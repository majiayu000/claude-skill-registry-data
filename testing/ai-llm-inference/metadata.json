{
  "name": "ai-llm-inference",
  "repo": "vasilyu1983/AI-Agents-public",
  "category": "testing",
  "description": "Operational patterns for LLM inference (recent advances): vLLM with 24x throughput gains, FP8/FP4 quantization (30-50% cost reduction), FlashInfer kernels, advanced fusions, PagedAttention, continuous",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:57:58.619605Z",
  "github_path": "frameworks/shared-skills/skills/ai-llm-inference",
  "github_branch": "main"
}