{
  "name": "llm-evaluation",
  "repo": "wshobson/agents",
  "category": "testing",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or est",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:55:34.607943Z",
  "github_path": "plugins/llm-application-dev/skills/llm-evaluation",
  "github_branch": "main",
  "dir_name": "llm-evaluation"
}