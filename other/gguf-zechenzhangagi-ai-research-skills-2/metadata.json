{
  "name": "gguf",
  "description": "GGUF format and llama.cpp quantization for efficient CPU/GPU inference. Use when deploying models on consumer hardware, Apple Silicon, or when needing flexible quantization from 2-8 bit without GPU re",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "path": "10-optimization/gguf",
  "category": "other",
  "tags": [],
  "stars": 0,
  "source": "github.com/zechenzhangAGI/AI-research-SKILLs",
  "dir_name": "gguf-zechenzhangagi-ai-research-skills-2"
}