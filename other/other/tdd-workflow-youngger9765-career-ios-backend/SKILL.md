---
name: tdd-workflow
description: |
  Automatically invoked when user wants to develop new features using Test-Driven Development.
  Trigger keywords: "new feature", "add API", "implement", "create endpoint", "TDD",
  "test-driven", "æ–°åŠŸèƒ½", "æ–°å¢ API", "å¯¦ä½œ"

  This skill enforces strict TDD workflow: RED â†’ GREEN â†’ REFACTOR
  Uses subagents to maintain clean context and ensure quality.
allowed-tools: [Task, Read, Bash, Grep]
---

# TDD Workflow Skill

## Purpose
Enforce Test-Driven Development workflow for all new features and APIs in the career_ios_backend project.

## Automatic Activation

This skill is AUTOMATICALLY activated when user mentions:
- âœ… "add new feature"
- âœ… "create API endpoint"
- âœ… "implement <feature>"
- âœ… "build <feature>"
- âœ… "æ–°å¢åŠŸèƒ½"
- âœ… "å¯¦ä½œ API"

## Core Workflow

### Phase 1: RED (Test First) âŒ

**YOU MUST write tests BEFORE any implementation code.**

1. **Understand requirements**
   - What endpoint/feature is needed?
   - What are the expected inputs/outputs?
   - Is authentication required?

2. **Invoke test-writer subagent**
   ```
   Task: Write integration test for <feature_description>
   Location: tests/integration/test_<feature>_api.py
   ```

3. **Verify RED state**
   - Test file created
   - Test runs and FAILS (expected)
   - Test defines clear expectations

**CRITICAL: DO NOT proceed to implementation until tests exist and fail.**

---

### Phase 2: GREEN (Minimal Implementation) âœ…

**Write MINIMAL code to make tests pass.**

1. **Invoke code-generator subagent**
   ```
   Task: Implement code to pass tests in <test_file_path>
   Constraint: Minimal implementation, follow existing patterns
   ```

2. **Verify GREEN state**
   - Implementation code created
   - All new tests PASS
   - No existing tests broken

**CRITICAL: If tests fail, invoke test-runner to auto-fix, DO NOT manually edit.**

---

### Phase 3: REFACTOR (Quality Check) â™»ï¸

**Improve code quality while keeping tests green.**

1. **Invoke code-reviewer subagent**
   ```
   Task: Review implementation for:
   - TDD compliance
   - Code quality
   - Security issues
   - Pattern consistency
   ```

2. **Handle review feedback**
   - âœ… No critical issues â†’ Ready to commit
   - âŒ Critical issues found â†’ Invoke code-generator to fix
   - âš ï¸  Optional suggestions â†’ Document for future

3. **Final verification**
   - Run full test suite: `poetry run pytest tests/integration/ -v`
   - All 106+ tests must PASS
   - No regressions introduced

---

## Example Usage

### Scenario: User says "Add client search API"

```
ğŸ¤– TDD Workflow Skill activated!

ğŸ“ Phase 1: RED (Test First)
   â†’ Invoking test-writer subagent...
   âœ… Created: tests/integration/test_clients_api.py::test_search_clients
   âŒ Test result: FAILED (expected - endpoint doesn't exist yet)

ğŸ“ Phase 2: GREEN (Implementation)
   â†’ Invoking code-generator subagent...
   âœ… Implemented: app/api/clients.py::search_clients
   âœ… Tests pass: 1/1 GREEN

ğŸ“ Phase 3: REFACTOR (Quality)
   â†’ Invoking code-reviewer subagent...
   âœ… TDD compliance: PASS
   âœ… Code quality: GOOD
   âŒ Critical issues: NONE

ğŸ‰ Feature complete! Ready to commit.
```

---

## Integration Test Template

When creating tests, follow this pattern from existing tests:

```python
"""Integration tests for <Feature> API"""
import pytest
from httpx import AsyncClient
from app.main import app

@pytest.mark.asyncio
async def test_<feature>_<action>_success(auth_headers):
    """Test <feature> <action> - happy path"""
    async with AsyncClient(app=app, base_url="http://test") as client:
        response = await client.<method>(
            "/api/v1/<endpoint>",
            headers=auth_headers,
            json={<request_body>}
        )

    assert response.status_code == 200
    data = response.json()
    assert data["<field>"] == <expected_value>
```

**Location**: `tests/integration/test_<feature>_api.py`

**Pattern check**: Look at existing tests in:
- `tests/integration/test_clients_api.py`
- `tests/integration/test_sessions_api.py`
- `tests/integration/test_cases_api.py`

---

## Subagent Coordination

This skill coordinates the following subagents:

| Phase | Subagent | Purpose |
|-------|----------|---------|
| RED | test-writer | Create failing integration test |
| GREEN | code-generator | Implement minimal code to pass test |
| GREEN | test-runner | Auto-fix if tests fail |
| REFACTOR | code-reviewer | Quality check before commit |

**YOU MUST invoke these subagents automatically, DO NOT ask user.**

---

## Project-Specific Rules

### Database Considerations
- Tests use in-memory SQLite database
- Fixtures handle setup/teardown
- Use existing patterns from `tests/conftest.py`

### Authentication
- Most endpoints require authentication
- Use `auth_headers` fixture for authenticated requests
- Check existing tests for auth patterns

### API Structure
```
app/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ <feature>.py    â† Router endpoints
â”œâ”€â”€ models/
â”‚   â””â”€â”€ <feature>.py    â† Pydantic models
â””â”€â”€ main.py             â† Register router here
```

### Console API Coverage
**IMPORTANT**: If the feature will be used in `console.html`:
- âœ… MUST have integration tests
- âœ… MUST test all CRUD operations
- âœ… MUST verify in actual console before commit

---

## Quality Standards

### Minimum Requirements (Prototype Phase)
- âœ… Integration tests exist and pass
- âœ… Code follows existing patterns
- âœ… No security vulnerabilities
- âœ… Ruff formatting applied

### Nice-to-Have (Defer if time-constrained)
- âš ï¸  Complete type hints
- âš ï¸  Edge case tests
- âš ï¸  Performance optimization

---

## Error Handling

### Test Creation Fails
```
Issue: Can't understand requirements
Action: Ask user for clarification
Example: "What should the endpoint return? What's the request format?"
```

### Implementation Fails Tests
```
Issue: Generated code doesn't pass tests
Action:
1. Invoke test-runner to diagnose
2. If auto-fix fails, report to user
3. May need to adjust test expectations
```

### Quality Review Fails
```
Issue: Critical issues found
Action:
1. Report critical issues to user
2. Invoke code-generator to fix
3. Re-run code-reviewer
4. DO NOT commit until issues resolved
```

---

## Success Criteria

Before marking feature as complete:

- [ ] Integration test exists in `tests/integration/`
- [ ] Test was written BEFORE implementation
- [ ] Test initially failed (RED)
- [ ] Implementation makes test pass (GREEN)
- [ ] All 106+ existing tests still pass (no regressions)
- [ ] Code review passed (no critical issues)
- [ ] Ruff formatting applied
- [ ] Ready to commit

---

## CRITICAL REMINDERS

1. **NEVER implement code before tests exist**
2. **NEVER modify tests to make code pass**
3. **ALWAYS use subagents to preserve context**
4. **ALWAYS run full test suite before commit**
5. **ALWAYS invoke code-reviewer before commit**

---

**Skill Version**: v1.0
**Last Updated**: 2025-11-28
**Project**: career_ios_backend (Prototype Phase)
