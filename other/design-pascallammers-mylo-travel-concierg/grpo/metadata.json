{
  "name": "grpo",
  "repo": "atrawog/bazzite-ai-plugins",
  "category": "design",
  "description": "Group Relative Policy Optimization for reinforcement learning from human feedback.Covers GRPOTrainer, reward function design, policy optimization, and KL divergenceconstraints for stable RLHF training",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T09:11:16.827849Z",
  "github_path": "bazzite-ai-jupyter/skills/grpo",
  "github_branch": "main"
}