{
  "name": "accelerate",
  "description": "Simplest distributed training API. 4 lines to add distributed support to any PyTorch script. Unified API for DeepSpeed/FSDP/Megatron/DDP. Automatic device placement, mixed precision (FP16/BF16/FP8). I",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "path": "08-distributed-training/accelerate",
  "category": "other",
  "tags": [],
  "stars": 0,
  "source": "github.com/zechenzhangAGI/AI-research-SKILLs",
  "dir_name": "accelerate-zechenzhangagi-ai-research-skills-2"
}