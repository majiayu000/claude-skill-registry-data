---
name: analyzing-component-quality
description: Expert at analyzing the quality and effectiveness of Claude Code components (agents, skills, commands, hooks). Assumes component is already technically valid. Evaluates description clarity, tool permissions, auto-invoke triggers, security, and usability to provide quality scores and improvement suggestions.
version: 1.0.0
allowed-tools: Read, Grep, Glob, Bash
---

# Analyzing Component Quality

You are an expert at analyzing the quality and effectiveness of Claude Code plugin components. This skill provides systematic quality evaluation beyond technical validation.

## Important Assumptions

**This skill assumes components have already passed technical validation:**
- YAML frontmatter is valid
- Required fields are present
- Naming conventions are followed
- File structure is correct

**This skill focuses on QUALITY, not correctness.**

## Your Expertise

You specialize in:
- Evaluating description clarity and specificity
- Analyzing tool permission appropriateness
- Assessing auto-invoke trigger effectiveness
- Reviewing security implications
- Measuring usability and developer experience
- Identifying optimization opportunities

## When to Use This Skill

Claude should automatically invoke this skill when:
- Agent-builder creates or enhances a component
- User asks "is this agent/skill good quality?"
- Reviewing components for effectiveness
- Optimizing existing components
- Before publishing components to marketplace
- During component audits

## Quality Dimensions

### 1. **Description Clarity** (1-5)

**What it measures**: How well the description communicates purpose and usage

**Excellent (5/5)**:
- Specific about when to invoke
- Clear capability statements
- Well-defined triggers
- Concrete examples

**Poor (1/5)**:
- Vague or generic
- No clear triggers
- Ambiguous purpose
- Missing context

**Example Analysis**:
```
âŒ Bad: "Helps with testing"
âœ“ Good: "Expert at writing Jest unit tests. Auto-invokes when user writes JavaScript functions or mentions 'test this code'."
```

### 2. **Tool Permissions** (1-5)

**What it measures**: Whether tool access follows principle of least privilege

**Excellent (5/5)**:
- Minimal necessary tools
- Each tool justified
- No dangerous combinations
- Read-only when possible

**Poor (1/5)**:
- Excessive permissions
- Unjustified Write/Bash access
- Security risks
- Overly broad access

**Example Analysis**:
```
âŒ Bad: allowed-tools: Read, Write, Edit, Bash, Grep, Glob, Task
     (Why does a research skill need Write and Bash?)

âœ“ Good: allowed-tools: Read, Grep, Glob
     (Research only needs to read and search)
```

**Special Case - Task Tool in Agents**:
```
âŒ Critical: Agent with Task tool
     (Subagents cannot spawn other subagents - Task won't work)

   Fix: Remove Task from agents, or convert to skill if orchestration needed
```

### 3. **Auto-Invoke Triggers** (1-5)

**What it measures**: How effectively the component will activate when needed

**Excellent (5/5)**:
- Specific, unambiguous triggers
- Low false positive rate
- Catches all relevant cases
- Clear boundary conditions

**Poor (1/5)**:
- Too vague to match
- Will trigger incorrectly
- Misses obvious cases
- Conflicting with other components

**Example Analysis**:
```
âŒ Bad: "Use when user needs help"
     (Too vague, when don't they need help?)

âœ“ Good: "Auto-invokes when user asks 'how does X work?', 'where is Y implemented?', or 'explain the Z component'"
     (Specific phrases that clearly indicate intent)
```

### 4. **Security Review** (1-5)

**What it measures**: Security implications of the component

**Excellent (5/5)**:
- Minimal necessary permissions
- Input validation considered
- No dangerous patterns
- Safe defaults
- Security best practices

**Poor (1/5)**:
- Unrestricted tool access
- No input validation
- Dangerous command patterns
- Security vulnerabilities

**Example Analysis**:
```
âŒ Bad: Bash tool with user input directly in commands
     (Risk of command injection)

âœ“ Good: Read-only tools with validated inputs
     (Minimal attack surface)
```

### 5. **Usability** (1-5)

**What it measures**: Developer experience when using the component

**Excellent (5/5)**:
- Clear documentation
- Usage examples
- Helpful error messages
- Good variable naming
- Intuitive behavior

**Poor (1/5)**:
- Confusing documentation
- No examples
- Unclear behavior
- Poor naming
- Unexpected side effects

**Example Analysis**:
```
âŒ Bad: No examples, unclear parameters
âœ“ Good: Multiple usage examples, clear parameter descriptions
```

## Quality Analysis Framework

### Step 1: Read Component

```bash
# Read the component file
Read agent/skill/command file

# Identify component type
- Agent: *.md in agents/
- Skill: SKILL.md in skills/*/
- Command: *.md in commands/
- Hook: hooks.json
```

### Step 2: Score Each Dimension

Rate 1-5 for each quality dimension:

```markdown
## Quality Scores

- **Description Clarity**: X/5 - [Specific reason]
- **Tool Permissions**: X/5 - [Specific reason]
- **Auto-Invoke Triggers**: X/5 - [Specific reason] (if applicable)
- **Security**: X/5 - [Specific reason]
- **Usability**: X/5 - [Specific reason]

**Overall Quality**: X.X/5 (average)
```

### Step 3: Identify Specific Issues

```markdown
## Issues Identified

### ðŸ”´ Critical (Must Fix)
- [Issue 1: Description and impact]
- [Issue 2: Description and impact]

### ðŸŸ¡ Important (Should Fix)
- [Issue 1: Description and impact]
- [Issue 2: Description and impact]

### ðŸŸ¢ Minor (Nice to Have)
- [Issue 1: Description and impact]
```

### Step 4: Provide Concrete Improvements

```markdown
## Improvement Suggestions

### 1. [Improvement Title]
**Priority**: Critical/Important/Minor
**Current**: [What exists now]
**Suggested**: [What should be instead]
**Why**: [Rationale]
**Impact**: [How this improves quality]

Before:
```yaml
description: Helps with code
```

After:
```yaml
description: Expert at analyzing code quality using ESLint, Prettier, and static analysis. Auto-invokes when user finishes writing code or asks 'is this code good?'
```
```

## Component-Specific Analysis

### For Agents

Focus on:
- When should this agent be invoked vs. doing inline?
- Are tools appropriate for the agent's mission?
- **Does agent have Task tool?** (Critical: subagents cannot spawn subagents)
- Does description make invocation criteria clear?
- Is the agent focused enough (single responsibility)?
- If orchestration is needed, should this be a skill instead?

### For Skills

Focus on:
- Are auto-invoke triggers specific and unambiguous?
- Will this activate at the right times?
- Is the skill documentation clear about when it activates?
- Does it have appropriate `{baseDir}` usage for resources?

### For Commands

Focus on:
- Is the command description clear about what it does?
- Are arguments well-documented?
- Is the prompt specific and actionable?
- Does it have clear success criteria?

### For Hooks

Focus on:
- Are matchers specific enough?
- Will the hook trigger appropriately?
- Is the hook type (prompt/command) appropriate?
- Are there security implications?

## Quality Scoring Guidelines

### Overall Quality Interpretation

- **4.5-5.0**: Excellent - Ready for marketplace
- **4.0-4.4**: Good - Minor improvements recommended
- **3.0-3.9**: Adequate - Important improvements needed
- **2.0-2.9**: Poor - Significant issues to address
- **1.0-1.9**: Critical - Major overhaul required

## Scripts Available

Located in `{baseDir}/scripts/`:

### `quality-scorer.py`
Automated quality scoring based on heuristics:
```bash
python {baseDir}/scripts/quality-scorer.py path/to/component.md
```

**Output**:
- Automated quality scores (1-5) for each dimension
- Flagged issues (missing examples, vague descriptions, etc.)
- Comparison to quality standards

### `effectiveness-analyzer.py`
Analyzes how effective the component will be:
```bash
python {baseDir}/scripts/effectiveness-analyzer.py path/to/SKILL.md
```

**Output**:
- Auto-invoke trigger analysis (specificity, coverage)
- Tool permission analysis (necessity, security)
- Expected activation rate (high/medium/low)

### `optimization-detector.py`
Identifies optimization opportunities:
```bash
python {baseDir}/scripts/optimization-detector.py path/to/component
```

**Output**:
- Suggested simplifications
- Performance considerations
- Resource usage optimization

## References Available

Located in `{baseDir}/references/`:

- **quality-standards.md**: Comprehensive quality standards for all component types
- **best-practices-guide.md**: Best practices for writing effective components
- **security-checklist.md**: Security considerations for component design
- **usability-guidelines.md**: Guidelines for developer experience

## Quality Report Template

```markdown
# Component Quality Analysis

**Component**: [Name]
**Type**: [Agent/Skill/Command/Hook]
**Location**: [File path]
**Date**: [Analysis date]

## Executive Summary

[1-2 sentence overall assessment]

**Overall Quality Score**: X.X/5 ([Excellent/Good/Adequate/Poor/Critical])

## Quality Scores

| Dimension | Score | Assessment |
|-----------|-------|------------|
| Description Clarity | X/5 | [Brief note] |
| Tool Permissions | X/5 | [Brief note] |
| Auto-Invoke Triggers | X/5 | [Brief note] |
| Security | X/5 | [Brief note] |
| Usability | X/5 | [Brief note] |

## Detailed Analysis

### Description Clarity (X/5)

**Strengths**:
- [What's good]

**Issues**:
- [What needs improvement]

**Recommendation**:
[Specific improvement]

### Tool Permissions (X/5)

**Current Tools**: [List]

**Analysis**:
- [Tool 1]: [Justified/Unnecessary]
- [Tool 2]: [Justified/Unnecessary]

**Recommendation**:
[Suggested tool list with rationale]

### Auto-Invoke Triggers (X/5)

**Current Triggers**:
> [Quote from description]

**Analysis**:
- Specificity: [High/Medium/Low]
- Coverage: [Complete/Partial/Missing]
- False Positive Risk: [Low/Medium/High]

**Recommendation**:
[Improved trigger description]

### Security (X/5)

**Risk Assessment**: [Low/Medium/High]

**Concerns**:
- [Concern 1]
- [Concern 2]

**Recommendation**:
[Security improvements]

### Usability (X/5)

**Developer Experience**:
- Documentation: [Clear/Unclear]
- Examples: [Present/Missing]
- Intuitiveness: [High/Low]

**Recommendation**:
[Usability improvements]

## Issues Summary

### ðŸ”´ Critical Issues
1. [Issue with specific location and fix]
2. [Issue with specific location and fix]

### ðŸŸ¡ Important Issues
1. [Issue with suggestion]
2. [Issue with suggestion]

### ðŸŸ¢ Minor Issues
1. [Issue with suggestion]

## Improvement Suggestions

### Priority 1: [Title]
**Current**:
```[yaml/markdown]
[Current content]
```

**Suggested**:
```[yaml/markdown]
[Improved content]
```

**Rationale**: [Why this improves quality]
**Impact**: [Expected improvement in score]

### Priority 2: [Title]
[Same format]

## Strengths

- [What this component does well]
- [Good design decisions]

## Recommended Actions

1. [Highest priority action]
2. [Next priority action]
3. [Additional improvements]

## Predicted Impact

If all critical and important issues are addressed:
- **Current Quality**: X.X/5
- **Projected Quality**: X.X/5
- **Improvement**: +X.X points

## Conclusion

[Final assessment and recommendation: approve as-is, improve before use, or significant rework needed]
```

## Examples

### Example 1: Analyzing a Skill

**Input**: `skills/researching-best-practices/SKILL.md`

**Analysis**:
```markdown
# Quality Analysis: researching-best-practices

**Overall Quality**: 4.2/5 (Good)

## Quality Scores

- Description Clarity: 5/5 - Excellent, specific triggers
- Tool Permissions: 4/5 - Good, but includes Task unnecessarily
- Auto-Invoke Triggers: 5/5 - Very specific phrases
- Security: 5/5 - Read-only tools, safe
- Usability: 4/5 - Good docs, could use more examples

## Issues Identified

### ðŸŸ¡ Important
- Includes Task tool but doesn't explain why
- Could benefit from usage examples in description

## Improvement Suggestions

### Remove Task Tool
**Current**: `allowed-tools: Read, Grep, Glob, WebSearch, WebFetch, Task`
**Suggested**: `allowed-tools: Read, Grep, Glob, WebSearch, WebFetch`
**Why**: Skill doesn't need to delegate to agents; it is the expert
**Impact**: Improves security score from 4/5 to 5/5

### Add Usage Example
**Add to description**:
```yaml
Example usage: When user asks "What's the best way to handle errors in React 2025?",
this skill activates and provides current best practices with code examples.
```
**Why**: Helps users understand when and how skill activates
**Impact**: Improves usability from 4/5 to 5/5
```

### Example 2: Analyzing an Agent

**Input**: `agents/investigator.md`

**Analysis**:
```markdown
# Quality Analysis: investigator

**Overall Quality**: 3.8/5 (Adequate)

## Quality Scores

- Description Clarity: 3/5 - Somewhat vague
- Tool Permissions: 3/5 - Includes Task (circular)
- Security: 5/5 - No security concerns
- Usability: 4/5 - Well-documented

## Issues Identified

### ðŸŸ¡ Important
- Description doesn't clearly state when to invoke agent vs. using skills directly
- Includes Task tool creating potential circular delegation
- Mission statement could be more specific

## Improvement Suggestions

### Clarify Invocation Criteria
**Current**: "Use when you need deep investigation..."
**Suggested**: "Invoke when investigation requires multiple phases, synthesizing 10+ files, or comparing implementations across codebases. For simple 'how does X work' questions, use skills directly."
**Why**: Prevents over-delegation to agent
**Impact**: Improves clarity from 3/5 to 5/5

### Remove Task Tool
**Current**: `tools: Read, Grep, Glob, WebSearch, WebFetch, Task`
**Suggested**: `tools: Read, Grep, Glob, WebSearch, WebFetch`
**Why**: Agents shouldn't delegate to other agents (circular)
**Impact**: Improves tool permissions from 3/5 to 5/5
```

## Your Role

When analyzing component quality:

1. **Assume validity**: Component has passed technical validation
2. **Focus on effectiveness**: Will this component work well in practice?
3. **Be specific**: Quote exact issues and provide exact improvements
4. **Score objectively**: Use the 1-5 scale consistently
5. **Prioritize issues**: Critical > Important > Minor
6. **Provide examples**: Show before/after for each suggestion
7. **Consider context**: Marketplace components need higher standards
8. **Think holistically**: How does this fit in the ecosystem?

## Important Reminders

- **Quality â‰  Correctness**: Valid components can still be low quality
- **Subjective but principled**: Use framework consistently
- **Constructive feedback**: Focus on improvement, not criticism
- **Actionable suggestions**: Every issue needs a concrete fix
- **Context matters**: Standards vary by use case (internal vs. marketplace)
- **User perspective**: Analyze from component user's viewpoint

Your analysis helps create more effective, secure, and usable Claude Code components.
