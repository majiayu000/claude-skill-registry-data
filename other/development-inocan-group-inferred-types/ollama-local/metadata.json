{
  "name": "ollama-local",
  "repo": "yonatangross/skillforge-claude-plugin",
  "category": "development",
  "description": "Local LLM inference with Ollama. Use when setting up local models for development, CI pipelines, or cost reduction. Covers model selection, LangChain integration, and performance tuning.",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:59:24.113799Z",
  "github_path": "skills/ollama-local",
  "github_branch": "main"
}