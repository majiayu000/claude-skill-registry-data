---
name: ai-unified-proxy
description: Unified AI Proxy - API multi-proveedor gratuita con rotaciÃ³n automÃ¡tica, streaming, y zero vendor lock-in
trigger: ai-proxy OR multi-provider OR free-tier OR ai-routing OR streaming OR unified-ai OR round-robin OR bun-api
scope: global
weight: 4.5
---

# AI Unified Proxy Architecture

## ğŸ¯ Objetivo

Crear una API unificada de IA que:

- âœ… No dependa de NADIE (zero vendor lock-in)
- âœ… Use capas gratuitas de mÃºltiples proveedores
- âœ… Rote servicios automÃ¡ticamente (round-robin)
- âœ… Soporte streaming de tokens (SSE)
- âœ… Sea extensible (local + cloud)
- âœ… Se despliegue fÃ¡cil en VPS (Coolify)

---

## ğŸ§± Stack TecnolÃ³gico

### Runtime: Bun (B)

**Por quÃ© Bun:**

- âš¡ Ultra rÃ¡pido (alternativa a Node.js)
- ğŸ“¦ `bun install` - gestor de paquetes integrado
- ğŸš€ `bun run` - ejecutor nativo
- ğŸ“˜ TypeScript nativo sin configuraciÃ³n
- ğŸ¯ Ideal para APIs livianas

**InstalaciÃ³n:**

```bash
curl -fsSL https://bun.sh/install | bash
```

### Servidor: Bun.serve()

**CaracterÃ­sticas:**

- HTTP server nativo (sin Express/Fastify)
- Puerto dinÃ¡mico: `process.env.PORT || 3000`
- Manejo manual de rutas (control total)
- Performance superior

---

## ğŸ§  Arquitectura LÃ³gica

```
Client
  |
POST /chat
  |
API Gateway (Bun)
  |
Round-Robin Selector
  |
AI Service Adapter (contrato comÃºn)
  |
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Grok   â”‚ Cerebras â”‚ Gemini  â”‚ OpenRouter â”‚  Local  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  |
Streaming Response (SSE)
```

**Flujo de datos:**

1. Cliente envÃ­a mensajes a `/chat`
2. Gateway valida y parsea JSON
3. Selector elige prÃ³ximo servicio (round-robin)
4. Servicio procesa con su SDK especÃ­fico
5. Tokens se streamean vÃ­a Server-Sent Events
6. Cliente recibe respuesta progresiva

---

## ğŸ“œ Contrato ComÃºn (Clave del Sistema)

### Tipo: ChatMessage

```typescript
type ChatMessage = {
  role: "system" | "user" | "assistant";
  content: string;
};
```

### Interfaz: AIService

```typescript
interface AIService {
  name: string;
  chat(messages: ChatMessage[]): AsyncIterable<string>;
}
```

### ğŸ‘‰ Regla de Oro

> **Todo proveedor que cumpla este contrato entra al sistema sin tocar el core.**

**Beneficios:**

- âœ… Adapter Pattern implementado
- âœ… Plug & Play de nuevos servicios
- âœ… Cero acoplamiento con proveedores
- âœ… Testing aislado por servicio

---

## ğŸ” Balanceo: Round Robin

### Estado Global

```typescript
const services: AIService[] = [];
let currentServiceIndex = 0;
```

### Selector

```typescript
function getNextService(): AIService {
  const service = services[currentServiceIndex];
  currentServiceIndex = (currentServiceIndex + 1) % services.length;
  return service;
}
```

### Ventajas

- âœ… Reparte carga equitativamente
- âœ… Extiende cuotas gratuitas
- âœ… Evita bloqueos por rate limit
- âœ… Sin estado complejo (stateless)

### EvoluciÃ³n Futura

```typescript
// Weighted Round Robin (segÃºn calidad/velocidad)
const services = [
  { service: grok, weight: 3 }, // 3x mÃ¡s frecuente
  { service: cerebras, weight: 2 }, // 2x
  { service: gemini, weight: 1 }, // 1x
];

// Health-aware routing
services.filter((s) => s.isHealthy());
```

---

## ğŸŒŠ Streaming (Clave Diferencial)

### TÃ©cnica: Async Generator + yield

```typescript
async function* streamResponse(service: AIService, messages: ChatMessage[]) {
  for await (const token of service.chat(messages)) {
    yield token;
  }
}
```

### Respuesta: Server-Sent Events (SSE)

**Headers requeridos:**

```typescript
{
  'Content-Type': 'text/event-stream',
  'Cache-Control': 'no-cache',
  'Connection': 'keep-alive'
}
```

**Formato de datos:**

```
data: {"token":"Hola"}\n\n
data: {"token":" mundo"}\n\n
data: {"done":true}\n\n
```

### Beneficios

- âš¡ Respuesta progresiva (tipo ChatGPT)
- ğŸ“‰ Menor latencia percibida
- ğŸ”— Compatible con frontend en tiempo real
- ğŸ¯ Backpressure natural

---

## ğŸ§© Servicios IA (Plugins)

### 1ï¸âƒ£ Grok (xAI)

**Free Tier:**

- ~60 req/min
- Modelos: Moonshot, Kimi K2

**ImplementaciÃ³n:**

```typescript
// services/grok.ts
import { OpenAI } from "openai";

const client = new OpenAI({
  apiKey: process.env.GROK_API_KEY,
  baseURL: "https://api.x.ai/v1",
});

export const grokService: AIService = {
  name: "Grok",
  async *chat(messages) {
    const stream = await client.chat.completions.create({
      model: "grok-beta",
      messages,
      stream: true,
    });

    for await (const chunk of stream) {
      const token = chunk.choices[0]?.delta?.content;
      if (token) yield token;
    }
  },
};
```

---

### 2ï¸âƒ£ Cerebras

**Free Tier:**

- 30 req/min
- 1M tokens/dÃ­a
- Modelos: GPT-OSS, Zhipu GLM-4.6

**CaracterÃ­sticas:**

- Muy rÃ¡pido en inferencia
- Excelente para tareas simples

**ImplementaciÃ³n:**

```typescript
// services/cerebras.ts
import { Cerebras } from "@cerebras/cerebras_cloud_sdk";

const client = new Cerebras({
  apiKey: process.env.CEREBRAS_API_KEY,
});

export const cerebrasService: AIService = {
  name: "Cerebras",
  async *chat(messages) {
    const stream = await client.chat.completions.create({
      model: "llama3.1-8b",
      messages,
      stream: true,
    });

    for await (const chunk of stream) {
      const token = chunk.choices[0]?.delta?.content;
      if (token) yield token;
    }
  },
};
```

---

### ğŸ”Œ Otros Servicios Compatibles

| Servicio            | Free Tier         | Velocidad  | Casos de uso           |
| ------------------- | ----------------- | ---------- | ---------------------- |
| **Gemini** (Google) | 15 RPM            | Media      | Razonamiento general   |
| **OpenRouter**      | Modelos gratis    | Variable   | Agregador multi-modelo |
| **Ollama**          | Ilimitado (local) | Depende HW | Offline, privacidad    |
| **LM Studio**       | Ilimitado (local) | Depende HW | UI + local             |

**Ejemplo Gemini:**

```typescript
import { GoogleGenerativeAI } from "@google/generative-ai";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

export const geminiService: AIService = {
  name: "Gemini",
  async *chat(messages) {
    const model = genAI.getGenerativeModel({ model: "gemini-pro" });
    const chat = model.startChat({ history: messages.slice(0, -1) });
    const result = await chat.sendMessageStream(
      messages[messages.length - 1].content,
    );

    for await (const chunk of result.stream) {
      yield chunk.text();
    }
  },
};
```

---

## ğŸ“ Estructura de Proyecto

```
ai-unified-proxy/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ grok.ts          # IntegraciÃ³n Grok/xAI
â”‚   â”œâ”€â”€ cerebras.ts      # IntegraciÃ³n Cerebras
â”‚   â”œâ”€â”€ gemini.ts        # IntegraciÃ³n Google Gemini
â”‚   â””â”€â”€ ollama.ts        # IntegraciÃ³n local Ollama
â”œâ”€â”€ types/
â”‚   â”œâ”€â”€ chat-message.ts  # Tipo ChatMessage
â”‚   â””â”€â”€ ai-service.ts    # Interfaz AIService
â”œâ”€â”€ index.ts             # API Gateway principal
â”œâ”€â”€ .env                 # Variables de entorno (NUNCA commitear)
â”œâ”€â”€ .env.example         # Template de .env
â”œâ”€â”€ package.json         # Dependencias
â”œâ”€â”€ nixpacks.toml        # Config de build Coolify
â”œâ”€â”€ tsconfig.json        # Config TypeScript
â””â”€â”€ README.md            # DocumentaciÃ³n
```

---

## âš™ï¸ ImplementaciÃ³n: index.ts (API Gateway)

### CÃ³digo completo:

```typescript
import { grokService } from "./services/grok";
import { cerebrasService } from "./services/cerebras";
import type { ChatMessage, AIService } from "./types/ai-service";

// Estado global
const services: AIService[] = [grokService, cerebrasService];
let currentServiceIndex = 0;

// Selector Round Robin
function getNextService(): AIService {
  const service = services[currentServiceIndex];
  currentServiceIndex = (currentServiceIndex + 1) % services.length;
  return service;
}

// Servidor HTTP
Bun.serve({
  port: process.env.PORT || 3000,

  async fetch(req) {
    const url = new URL(req.url);

    // Ruta: POST /chat
    if (req.method === "POST" && url.pathname === "/chat") {
      try {
        // Parse body
        const body = await req.json();
        const messages = body.messages as ChatMessage[];

        // ValidaciÃ³n bÃ¡sica
        if (!Array.isArray(messages) || messages.length === 0) {
          return new Response("Invalid messages", { status: 400 });
        }

        // Seleccionar servicio
        const service = getNextService();
        console.log(`[${new Date().toISOString()}] Using: ${service.name}`);

        // Stream de respuesta
        const stream = new ReadableStream({
          async start(controller) {
            try {
              for await (const token of service.chat(messages)) {
                const data = `data: ${JSON.stringify({ token })}\n\n`;
                controller.enqueue(new TextEncoder().encode(data));
              }

              // Mensaje de finalizaciÃ³n
              controller.enqueue(
                new TextEncoder().encode(
                  `data: ${JSON.stringify({ done: true })}\n\n`,
                ),
              );
            } catch (error) {
              console.error(`Error with ${service.name}:`, error);
              controller.enqueue(
                new TextEncoder().encode(
                  `data: ${JSON.stringify({ error: error.message })}\n\n`,
                ),
              );
            } finally {
              controller.close();
            }
          },
        });

        // Respuesta SSE
        return new Response(stream, {
          headers: {
            "Content-Type": "text/event-stream",
            "Cache-Control": "no-cache",
            Connection: "keep-alive",
          },
        });
      } catch (error) {
        return new Response(`Error: ${error.message}`, { status: 500 });
      }
    }

    // Fallback
    return new Response("Not Found", { status: 404 });
  },
});

console.log(`ğŸš€ AI Proxy running on port ${process.env.PORT || 3000}`);
```

---

## ğŸ” Variables de Entorno

### `.env` (NUNCA commitear)

```bash
# API Keys
GROK_API_KEY=xai-xxxxxxxxxxxxx
CEREBRAS_API_KEY=csk-xxxxxxxxxxxxx
GEMINI_API_KEY=AIzaxxxxxxxxxxxxx

# Server
PORT=3000
NODE_ENV=production
```

### `.env.example` (Template)

```bash
# API Keys (obtener en respectivos dashboards)
GROK_API_KEY=your_grok_key_here
CEREBRAS_API_KEY=your_cerebras_key_here
GEMINI_API_KEY=your_gemini_key_here

# Server config
PORT=3000
NODE_ENV=production
```

### âš ï¸ Buenas PrÃ¡cticas

- Solo runtime, no buildtime
- Usar `.gitignore` para `.env`
- Nunca hardcodear keys
- Validar keys al inicio

---

## ğŸš€ Deploy en Coolify

### Paso 1: Preparar VPS

**Requisitos mÃ­nimos:**

- 2 vCPU
- 8 GB RAM (4 GB suficiente)
- 20 GB disco
- Ubuntu 22.04 LTS

**Proveedores recomendados:**

- Hostinger VPS (~$5/mes)
- Hetzner Cloud (~â‚¬4/mes)
- DigitalOcean Droplets (~$6/mes)

### Paso 2: Instalar Coolify

```bash
curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash
```

**Acceder:** `http://your-vps-ip:8000`

### Paso 3: Configurar proyecto en Coolify

1. **New Resource** â†’ **Git Repository**
2. Conectar tu repo (GitHub/GitLab)
3. Configurar:
   - **Build Pack**: Nixpacks
   - **Port**: 3000
   - **Environment Variables**: Agregar API keys

### Paso 4: nixpacks.toml

```toml
[setup]
packages = ["bun"]

[install]
cmds = ["bun install"]

[start]
cmd = "bun run index.ts"

[variables]
PORT = "3000"
```

### Paso 5: Deploy

- Coolify detecta cambios en Git
- Build automÃ¡tico con Nixpacks
- Deploy con zero-downtime
- Logs en vivo en UI

---

## ğŸ§ª Testing

### Testing Local

```bash
# Instalar dependencias
bun install

# Crear .env con tus keys
cp .env.example .env
# (editar .env con tus API keys)

# Correr servidor
bun run index.ts
```

### Testing con curl

```bash
curl -N -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages":[
      {"role":"user","content":"Explica Fibonacci en JavaScript"}
    ]
  }'
```

**Resultado esperado:**

```
data: {"token":"AquÃ­"}

data: {"token":" tienes"}

data: {"token":" una"}

data: {"token":" implementaciÃ³n"}
...
data: {"done":true}
```

### Testing con JavaScript

```javascript
const eventSource = new EventSource("/chat", {
  method: "POST",
  body: JSON.stringify({
    messages: [{ role: "user", content: "Hola mundo" }],
  }),
});

eventSource.onmessage = (event) => {
  const data = JSON.parse(event.data);
  if (data.done) {
    eventSource.close();
  } else {
    console.log(data.token);
  }
};
```

---

## ğŸ§  MetodologÃ­as ImplÃ­citas

| Pattern                     | DescripciÃ³n                        | Beneficio      |
| --------------------------- | ---------------------------------- | -------------- |
| **Adapter Pattern**         | Cada proveedor es un plugin        | Extensibilidad |
| **Gateway Pattern**         | API Ãºnica para mÃºltiples servicios | Simplicidad    |
| **Backpressure Natural**    | Streaming controla flujo           | Performance    |
| **Fail Soft**               | RotaciÃ³n automÃ¡tica                | Resiliencia    |
| **Cost-Aware Architecture** | Gratis primero, pago Ãºltimo        | EconomÃ­a       |

---

## ğŸ”® Extensiones Naturales

### 1. Weighted Round Robin

```typescript
const services = [
  { service: grok, weight: 3, quality: 0.9 },
  { service: cerebras, weight: 2, quality: 0.85 },
  { service: gemini, weight: 1, quality: 0.95 },
];

// Selector basado en peso y calidad
function getWeightedService() {
  const totalWeight = services.reduce((sum, s) => sum + s.weight, 0);
  let random = Math.random() * totalWeight;

  for (const item of services) {
    random -= item.weight;
    if (random <= 0) return item.service;
  }
}
```

### 2. Health Checks

```typescript
interface AIService {
  name: string;
  chat(messages: ChatMessage[]): AsyncIterable<string>;
  healthCheck(): Promise<boolean>; // Nuevo
}

async function getHealthyService() {
  const healthy = await Promise.all(
    services.map(async (s) => ({
      service: s,
      isHealthy: await s.healthCheck(),
    })),
  );
  return healthy.find((h) => h.isHealthy)?.service;
}
```

### 3. Fallback Local AutomÃ¡tico

```typescript
const services = [
  grokService,
  cerebrasService,
  geminiService,
  ollamaService, // Ãšltimo recurso (local, siempre available)
];
```

### 4. Cache SemÃ¡ntica

```typescript
import { similarity } from "ml-distance";

const cache = new Map<string, string>();

function getCachedResponse(prompt: string) {
  for (const [cachedPrompt, response] of cache) {
    if (similarity(prompt, cachedPrompt) > 0.95) {
      return response;
    }
  }
  return null;
}
```

### 5. Rate Limit Aware Routing

```typescript
interface AIService {
  name: string;
  rateLimitRemaining: number;
  rateLimitReset: Date;
  chat(messages: ChatMessage[]): AsyncIterable<string>;
}

function getRateLimitSafeService() {
  return services.find((s) => s.rateLimitRemaining > 0) || services[0];
}
```

### 6. Observabilidad

```typescript
import { logger } from "./logger";

for await (const token of service.chat(messages)) {
  logger.info({
    service: service.name,
    timestamp: Date.now(),
    tokenLength: token.length,
  });
  yield token;
}
```

### 7. Policy Engine

```typescript
function selectServiceByTask(task: string) {
  if (task.includes("cÃ³digo")) return cerebrasService; // RÃ¡pido
  if (task.includes("razonamiento")) return geminiService; // Inteligente
  return grokService; // Default
}
```

---

## ğŸš¨ Troubleshooting

### Problema: "No streaming"

**Causa:** Headers SSE incorrectos  
**SoluciÃ³n:**

```typescript
{
  'Content-Type': 'text/event-stream',  // No 'application/json'
  'Cache-Control': 'no-cache',
  'Connection': 'keep-alive'
}
```

### Problema: "Rate limit exceeded"

**Causa:** Todas las APIs agotadas  
**SoluciÃ³n:** Agregar mÃ¡s proveedores o fallback local

### Problema: "Bun not found en Coolify"

**Causa:** nixpacks.toml incorrecto  
**SoluciÃ³n:**

```toml
[setup]
packages = ["bun"]  # Asegurar que estÃ© presente
```

### Problema: "CORS errors"

**SoluciÃ³n:**

```typescript
headers: {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
  'Content-Type': 'text/event-stream',
  // ...
}
```

---

## ğŸ“š Related Skills

- `cloud-ready` - Deploy y gestiÃ³n en VPS
- `ai-security` - ProtecciÃ³n contra prompt injection
- `conventional-commits` - Versionado semÃ¡ntico
- `machine-health` - Monitoreo de recursos

---

## ğŸ§  Resumen Brutal (1 LÃ­nea)

> **Una API de IA modular, gratuita, extensible y sin vendor lock-in, basada en contratos, streaming y rotaciÃ³n inteligente de proveedores.**

---

**Ready to deploy. ğŸš€**
