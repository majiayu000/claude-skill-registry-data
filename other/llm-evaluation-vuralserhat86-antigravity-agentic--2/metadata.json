{
  "name": "llm_evaluation",
  "description": "Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM performance, measuring AI application quality, or est",
  "repo": "vuralserhat86/antigravity-agentic-skills",
  "path": "skills/llm_evaluation",
  "category": "other",
  "tags": [],
  "stars": 0,
  "source": "github.com/vuralserhat86/antigravity-agentic-skills",
  "dir_name": "llm-evaluation-vuralserhat86-antigravity-agentic--2"
}