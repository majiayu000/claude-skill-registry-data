{
  "name": "qlora",
  "repo": "itsmostafa/llm-engineering-skills",
  "category": "development",
  "description": "Memory-efficient fine-tuning with 4-bit quantization and LoRA adapters. Use when fine-tuning large models (7B+) on consumer GPUs, when VRAM is limited, or when standard LoRA still exceeds memory. Buil",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T09:03:35.274806Z",
  "github_path": "skills/qlora",
  "github_branch": "main",
  "dir_name": "qlora"
}