{
  "name": "llamafile",
  "description": "When setting up local LLM inference without cloud APIs. When running GGUF models locally. When needing OpenAI-compatible API from a local model. When building offline/air-gapped AI tools. When trouble",
  "repo": "Jamie-BitFlight/claude_skills",
  "category": "development",
  "tags": [
    "llamafile"
  ],
  "stars": 4,
  "github_path": "plugins/llamafile/skills/llamafile",
  "github_branch": "main"
}