{
  "name": "long-context",
  "description": "Extend context windows of transformer models using RoPE, YaRN, ALiBi, and position interpolation techniques. Use when processing long documents (32k-128k+ tokens), extending pre-trained models beyond ",
  "category": "development",
  "source": "github.com/davila7/claude-code-templates",
  "imported_at": "2026-01-24T14:54:19.704855Z",
  "dir_name": "long-context"
}