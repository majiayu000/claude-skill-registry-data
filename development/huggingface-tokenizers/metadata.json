{
  "name": "huggingface-tokenizers",
  "description": "Fast tokenizers optimized for research and production. Rust-based implementation tokenizes 1GB in <20 seconds. Supports BPE, WordPiece, and Unigram algorithms. Train custom vocabularies, track alignme",
  "category": "development",
  "source": "github.com/davila7/claude-code-templates",
  "imported_at": "2026-01-24T14:54:19.830944Z",
  "dir_name": "huggingface-tokenizers"
}