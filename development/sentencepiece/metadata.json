{
  "name": "sentencepiece",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "development",
  "description": "Language-independent tokenizer treating text as raw Unicode. Supports BPE and Unigram algorithms. Fast (50k sentences/sec), lightweight (6MB memory), deterministic vocabulary. Used by T5, ALBERT, XLNe",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:18.123689Z",
  "github_path": "02-tokenization/sentencepiece",
  "github_branch": "main"
}