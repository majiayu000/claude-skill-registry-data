{
  "name": "llm-cost-optimization",
  "repo": "ebiyy/traylingo",
  "category": "development",
  "description": "Reduce LLM API costs without sacrificing quality. Covers prompt caching (Anthropic), local response caching, prompt compression, debouncing triggers, and cost analysis. Use when building LLM-powered f",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T09:05:20.298243Z",
  "github_path": ".claude/skills/llm-cost-optimization",
  "github_branch": "main"
}