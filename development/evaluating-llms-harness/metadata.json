{
  "name": "evaluating-llms-harness",
  "description": "Evaluates LLMs across 60+ academic benchmarks (MMLU, HumanEval, GSM8K, TruthfulQA, HellaSwag). Use when benchmarking model quality, comparing models, reporting academic results, or tracking training p",
  "category": "development",
  "source": "github.com/davila7/claude-code-templates",
  "imported_at": "2026-01-24T14:54:19.772766Z",
  "dir_name": "evaluating-llms-harness"
}