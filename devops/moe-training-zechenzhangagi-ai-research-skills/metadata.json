{
  "name": "moe-training",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Train Mixture of Experts (MoE) models using DeepSpeed or HuggingFace. Use when training large-scale models with limited compute (5Ã— cost reduction vs dense models), implementing sparse architectures l",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:20.102866Z",
  "github_path": "19-emerging-techniques/moe-training",
  "github_branch": "main",
  "dir_name": "moe-training-zechenzhangagi-ai-research-skills"
}