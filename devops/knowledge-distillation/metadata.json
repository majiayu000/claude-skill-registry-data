{
  "name": "knowledge-distillation",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Compress large language models using knowledge distillation from teacher to student models. Use when deploying smaller models with retained performance, transferring GPT-4 capabilities to open-source ",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:19.914911Z",
  "github_path": "19-emerging-techniques/knowledge-distillation",
  "github_branch": "main",
  "dir_name": "knowledge-distillation"
}