{
  "name": "tensorrt-llm",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Optimizes LLM inference with NVIDIA TensorRT for maximum throughput and lowest latency. Use for production deployment on NVIDIA GPUs (A100/H100), when you need 10-100x faster inference than PyTorch, o",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:19.243713Z",
  "github_path": "12-inference-serving/tensorrt-llm",
  "github_branch": "main",
  "dir_name": "tensorrt-llm"
}