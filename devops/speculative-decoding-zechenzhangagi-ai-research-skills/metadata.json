{
  "name": "speculative-decoding",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Accelerate LLM inference using speculative decoding, Medusa multiple heads, and lookahead decoding techniques. Use when optimizing inference speed (1.5-3.6Ã— speedup), reducing latency for real-time ap",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:20.159554Z",
  "github_path": "19-emerging-techniques/speculative-decoding",
  "github_branch": "main",
  "dir_name": "speculative-decoding-zechenzhangagi-ai-research-skills"
}