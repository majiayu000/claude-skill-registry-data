---
name: python_sandbox
description: åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡ŒPythonä»£ç ï¼Œç”¨äºæ•°æ®åˆ†æã€å¯è§†åŒ–å’Œç”ŸæˆExcelã€Wordã€PDFç­‰æ–‡ä»¶ã€‚æ”¯æŒæ•°æ®æ¸…æ´—ã€ç»Ÿè®¡åˆ†æã€æœºå™¨å­¦ä¹ ã€å›¾è¡¨ç”Ÿæˆã€æ–‡æ¡£è‡ªåŠ¨åŒ–ç­‰å¤æ‚å·¥ä½œæµã€‚
tool_name: python_sandbox
category: code
priority: 10
tags: ["python", "code", "visualization", "data-analysis", "chart", "document", "automation", "machine-learning", "reporting", "excel", "word", "pdf", "ppt"]
version: 2.5
references: ["matplotlib_cookbook.md", "pandas_cheatsheet.md", "report_generator_workflow.md", "ml_workflow.md", "sympy_cookbook.md","scipy_cookbook.md", "text_analysis_cookbook.md"]
---

# Pythonæ²™ç›’å·¥å…·ä½¿ç”¨æŒ‡å— v2.5 (ä¸åç«¯å®Œå…¨åŒ¹é…ç‰ˆ)

## ğŸ¯ **æ ¸å¿ƒèƒ½åŠ›æ¦‚è§ˆ**

Pythonæ²™ç›’æ˜¯ä¸€ä¸ª**å¤šåŠŸèƒ½çš„ä»£ç æ‰§è¡Œç¯å¢ƒ**ï¼Œæ”¯æŒï¼š

| åŠŸèƒ½é¢†åŸŸ | ä¸»è¦ç”¨é€” | å…³é”®åº“ |
|---------|---------|-------|
| **æ•°æ®åˆ†æ** | æ•°æ®æ¸…æ´—ã€è½¬æ¢ã€èšåˆ | Pandas, Polars |
| **é«˜æ€§èƒ½è®¡ç®—** | å†…å­˜SQLã€è¡¨è¾¾å¼åŠ é€Ÿ | DuckDB, Numexpr, Bottleneck |
| **å¯è§†åŒ–** | å›¾è¡¨ç”Ÿæˆä¸è‡ªåŠ¨æ•è· | Matplotlib, Seaborn |
| **æ–‡æ¡£è‡ªåŠ¨åŒ–** | Excel/Word/PDF/PPTç”Ÿæˆ | python-docx, reportlab, openpyxl |
| **æœºå™¨å­¦ä¹ ** | æ¨¡å‹è®­ç»ƒä¸è¯„ä¼° | scikit-learn, LightGBM |
| **ç¬¦å·æ•°å­¦** | å…¬å¼è¯æ˜ã€æ–¹ç¨‹æ±‚è§£ | SymPy |
| **ç§‘å­¦è®¡ç®—** | ä¼˜åŒ–ã€ç§¯åˆ†ã€ä¿¡å·å¤„ç† | SciPy |
| **æµç¨‹å›¾ç”Ÿæˆ** | æ¶æ„å›¾ã€æµç¨‹å›¾ | Graphviz, NetworkX |
| **æ–‡æœ¬åˆ†æ** | HTMLè§£æã€æ•°æ®æå– | BeautifulSoup4, lxml |
| **æ€§èƒ½ä¼˜åŒ–** | æœºæ¢°ç¡¬ç›˜ä¼˜åŒ–ã€å¼‚æ­¥IO | aiofiles, joblib |


---

## ğŸ“ **æ–‡ä»¶å¤„ç†æŒ‡å— - ä¸¤ç§æ¨¡å¼å¿…é¡»åˆ†æ¸…**

### **æ¨¡å¼A: å·¥ä½œåŒºæ–‡ä»¶ (`/data` ç›®å½•)**
**ç”¨é€”**: æ•°æ®åˆ†æã€å¤„ç†ã€æŒä¹…åŒ–å­˜å‚¨  
**æ”¯æŒæ ¼å¼**: `.csv`, `.xlsx`, `.xls`, `.parquet`, `.json`, `.txt`, `.feather`  
**è®¿é—®æ–¹å¼**: ç»å¯¹è·¯å¾„ `/data/æ–‡ä»¶å`  
```python
import pandas as pd
df = pd.read_csv('/data/sales.csv')  # âœ… æ­£ç¡®
```

### **æ¨¡å¼B: ä¸Šä¸‹æ–‡æ–‡ä»¶ (Base64åµŒå…¥)**
**ç”¨é€”**: å›¾ç‰‡è¯†åˆ«ã€PDFå†…å®¹æå–  
**æ”¯æŒæ ¼å¼**: `.png`, `.jpg`, `.jpeg`, `.pdf`, `.txt`(å°æ–‡ä»¶)  
**ç‰¹ç‚¹**: æ–‡ä»¶å†…å®¹ç›´æ¥åµŒå…¥å¯¹è¯ï¼Œ**ä¸åœ¨ `/data` ç›®å½•**
```python
# âŒ é”™è¯¯ï¼šæ— æ³•ä»/dataè¯»å–ä¸Šä¼ çš„å›¾ç‰‡
# img = Image.open('/data/uploaded_image.png')  # ä¼šå¤±è´¥
```

---

## ğŸš€ **è¾“å‡ºè§„èŒƒ - åç«¯å®é™…æ”¯æŒçš„æ ¼å¼**

### **1. å›¾è¡¨è¾“å‡º - ç³»ç»Ÿè‡ªåŠ¨æ•è·**
```python
import matplotlib.pyplot as plt
plt.plot([1,2,3], [4,5,6])
plt.title('ç¤ºä¾‹å›¾è¡¨')
plt.show()  # ğŸ¯ å…³é”®ï¼šè‡ªåŠ¨æ•è·ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†

# æ”¯æŒä»¥ä¸‹å›¾è¡¨åº“çš„è‡ªåŠ¨æ•è·ï¼š
# - Matplotlib (ä½¿ç”¨ plt.show() è§¦å‘)
# - Graphviz (åˆ›å»º Digraph å¯¹è±¡è‡ªåŠ¨æ•è·)
# - NetworkX (é€šè¿‡ Matplotlib æ¸²æŸ“)
```

### **2. å¯ä¸‹è½½æ–‡ä»¶ - å¿…é¡»ä½¿ç”¨JSONæ ¼å¼**
```python
import base64
import json

# ç”Ÿæˆæ–‡ä»¶å†…å®¹å...
file_data = base64.b64encode(content).decode('utf-8')

# ğŸ¯ åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºç±»å‹ï¼š
output = {
    "type": "excel",  # æˆ– "word", "pdf", "ppt"
    "title": "é”€å”®æŠ¥å‘Š.xlsx",
    "data_base64": file_data  # æ³¨æ„ï¼šåªæœ‰imageç±»å‹ç”¨"image_base64"
}

# å¯¹äºå›¾ç‰‡è¾“å‡ºï¼Œåç«¯è‡ªåŠ¨ç”Ÿæˆï¼š
# {
#   "type": "image",
#   "title": "å›¾è¡¨æ ‡é¢˜",
#   "image_base64": "base64å­—ç¬¦ä¸²"
# }

print(json.dumps(output))  # ğŸ¯ å¿…é¡»ç”¨JSONæ ¼å¼æ‰“å°
```

### **3. æ–‡æœ¬/æ•°æ® - ç›´æ¥print**
```python
print("åˆ†æç»“æœ:")
print(f"æ€»è®¡: {total}")
print(df.describe())  # Pandas DataFrameè‡ªåŠ¨ç¾åŒ–æ˜¾ç¤º
```

### **åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºç±»å‹åˆ—è¡¨ï¼š**
- `"image"` - å›¾è¡¨ã€æµç¨‹å›¾ï¼ˆè‡ªåŠ¨æ•è·ï¼‰
- `"excel"` - Excelæ–‡ä»¶
- `"word"` - Wordæ–‡æ¡£
- `"pdf"` - PDFæ–‡ä»¶
- `"ppt"` - PowerPointæ¼”ç¤ºæ–‡ç¨¿

---

## ğŸ’¾ **ä¼šè¯æŒä¹…åŒ– - è·¨ä»£ç æ‰§è¡Œçš„æ–‡ä»¶å…±äº«**

### **ä¼šè¯æœºåˆ¶ï¼š**
- **ä¼šè¯ID**: æ¯ä¸ªä¼šè¯æœ‰å”¯ä¸€IDï¼Œæ–‡ä»¶æŒ‰ä¼šè¯éš”ç¦»
- **è¶…æ—¶æ—¶é—´**: 24å°æ—¶æ— æ´»åŠ¨è‡ªåŠ¨æ¸…ç†
- **å·¥ä½œç›®å½•**: `/data` ç›®å½•å¯¹åº”ä¼šè¯å·¥ä½œåŒº

### **å·¥ä½œæµç¤ºä¾‹ï¼š**
```python
# ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ•°æ®å¹¶ä¿å­˜
import pandas as pd
df = pd.read_excel('/data/åŸå§‹æ•°æ®.xlsx')
processed = df.groupby('éƒ¨é—¨')['é”€å”®é¢'].sum()
processed.to_csv('/data/éƒ¨é—¨æ±‡æ€».csv')  # âœ… ä¿å­˜ä¸­é—´ç»“æœ
print("å·²ä¿å­˜éƒ¨é—¨æ±‡æ€»æ•°æ®")

# ç¬¬äºŒæ­¥ï¼šè¯»å–ä¸­é—´ç»“æœç»§ç»­åˆ†æ
df_summary = pd.read_csv('/data/éƒ¨é—¨æ±‡æ€».csv')
print(f"è¯»å–åˆ° {len(df_summary)} ä¸ªéƒ¨é—¨çš„æ±‡æ€»æ•°æ®")
```

### **é‡è¦æé†’ï¼š**
- âœ… åŒä¸€ä¼šè¯å†…æ–‡ä»¶æŒä¹…åŒ–ï¼ˆ24å°æ—¶è¶…æ—¶ï¼‰
- âœ… æ–°ä¼šè¯å¼€å§‹æ—¶ `/data` ç›®å½•ä¸ºç©º
- âœ… å»ºè®®ä¿å­˜ä¸­é—´ç»“æœé¿å…é‡å¤è®¡ç®—
- âœ… ä½¿ç”¨åŒä¸€session_idå¯è·¨å¤šæ¬¡ä»£ç æ‰§è¡Œå…±äº«æ–‡ä»¶

---

## ğŸ“š **å·¥ä½œæµå‚è€ƒ - æŒ‰éœ€æŸ¥é˜…**

### **å¿«é€ŸæŸ¥æ‰¾è¡¨ï¼š**

| ä»»åŠ¡ç±»å‹ | å‚è€ƒæ–‡ä»¶ | æ ¸å¿ƒåº“ |
|---------|---------|-------|
| **åˆ›å»ºå›¾è¡¨** | `matplotlib_cookbook.md` | matplotlib, seaborn |
| **æ•°æ®å¤„ç†** | `pandas_cheatsheet.md` | pandas, duckdb |
| **ç”ŸæˆæŠ¥å‘Š** | `report_generator_workflow.md` | python-docx, reportlab |
| **æœºå™¨å­¦ä¹ ** | `ml_workflow.md` | scikit-learn, lightgbm |
| **ç¬¦å·æ•°å­¦** | `sympy_cookbook.md` | sympy |
| **ç§‘å­¦è®¡ç®—** | `scipy_cookbook.md` | scipy |
| **æ–‡æœ¬è§£æ** | `text_analysis_cookbook.md` | beautifulsoup4, lxml |

### **ç¤ºä¾‹å·¥ä½œæµï¼š**

#### **A. å…¬å¼è¯æ˜å·¥ä½œæµ**
```python
# 1. å®šä¹‰ç¬¦å·
import sympy as sp
x, y = sp.symbols('x y')

# 2. æ„å»ºè¡¨è¾¾å¼
lhs = (x + y)**2
rhs = x**2 + 2*x*y + y**2

# 3. éªŒè¯æ’ç­‰
difference = sp.simplify(lhs - rhs)
print(f"å·®å€¼: {difference}")
print(f"æ˜¯å¦æ’ç­‰: {difference == 0}")
```

#### **B. ETLæ•°æ®åˆ†æå·¥ä½œæµ**
```python
# Extract
df = pd.read_csv('/data/raw.csv')

# Transform
df_clean = (df
           .dropna()
           .drop_duplicates()
           .assign(profit = lambda d: d['revenue'] - d['cost']))

# Load
df_clean.to_csv('/data/cleaned.csv', index=False)
print(df_clean.describe())
```

#### **C. Graphvizæµç¨‹å›¾ç”Ÿæˆ**
```python
from graphviz import Digraph

# åˆ›å»ºæµç¨‹å›¾
dot = Digraph(comment='å·¥ä½œæµç¨‹', format='png')
dot.node('A', 'æ•°æ®é‡‡é›†')
dot.node('B', 'æ•°æ®æ¸…æ´—')
dot.node('C', 'æ•°æ®åˆ†æ')
dot.node('D', 'æŠ¥å‘Šç”Ÿæˆ')

dot.edges(['AB', 'BC', 'CD'])
dot.attr(rankdir='LR')  # ä»å·¦åˆ°å³å¸ƒå±€

# ğŸ¯ è‡ªåŠ¨æ•è·ï¼šGraphvizå›¾è¡¨ä¼šè¢«åç«¯è‡ªåŠ¨æ•è·å¹¶è¾“å‡ºä¸ºå›¾ç‰‡
```

---

## âš¡ **æ€§èƒ½ä¼˜åŒ–æŒ‡å— (ä¸åç«¯å®Œå…¨åŒ¹é…)**

### **1. åç«¯èµ„æºé…ç½®**
```yaml
å†…å­˜é™åˆ¶: 6GB (mem_limit: "6g")
é¢„ç•™å†…å­˜: 4GB (mem_reservation: "4g")
Swapé™åˆ¶: ç¦ç”¨ (memswap_limit: "0")  # ğŸ”¥ é¿å…æœºæ¢°ç¡¬ç›˜swapæ­»æœº
CPUé™åˆ¶: 75%é…é¢ (cpu_quota: 75_000, cpu_period: 100_000)
è¶…æ—¶æ—¶é—´: 90ç§’
æ–‡ä»¶ç³»ç»Ÿ: åªè¯»æ ¹ç›®å½•ï¼Œ/dataå¯å†™ï¼Œ/tmpä¸ºtmpfs
ç½‘ç»œ: å®Œå…¨ç¦ç”¨ (network_disabled: true)
```

### **2. å¤§æ–‡ä»¶å¤„ç†ç­–ç•¥**

#### **åˆ†å—è¯»å– (50MB+æ–‡ä»¶)**
```python
chunks = []
for chunk in pd.read_csv('/data/large.csv', chunksize=50000):
    processed = process_chunk(chunk)  # è‡ªå®šä¹‰å¤„ç†å‡½æ•°
    chunks.append(processed)
final_df = pd.concat(chunks, ignore_index=True)
```

#### **æ ¼å¼è½¬æ¢åŠ é€Ÿ**
```python
# è½¬æ¢CSVä¸ºFeatheræ ¼å¼ (æé€Ÿ10-100å€)
import pyarrow.feather as feather
df = pd.read_csv('/data/slow.csv')
feather.write_feather(df, '/data/fast.feather')  # ä¿å­˜

# åç»­è¯»å–æå¿«
df_fast = feather.read_feather('/data/fast.feather')
```

### **3. å†…å­˜å¤–è®¡ç®— (é¿å…OOM)**

#### **DuckDBå†…å­˜SQL**
```python
import duckdb

# ç›´æ¥æŸ¥è¯¢CSVï¼Œä¸åŠ è½½åˆ°å†…å­˜
result = duckdb.sql("""
    SELECT department, 
           AVG(salary) as avg_salary,
           COUNT(*) as count
    FROM read_csv_auto('/data/employees.csv')
    WHERE hire_date > '2024-01-01'
    GROUP BY department
    ORDER BY avg_salary DESC
    LIMIT 10
""").df()  # æœ€åè½¬ä¸ºDataFrame
print(result)
```

#### **Numexprè¡¨è¾¾å¼åŠ é€Ÿ**
```python
import numexpr as ne

# ä¼ ç»Ÿæ–¹å¼ï¼ˆæ…¢ï¼‰
df['result'] = df['A'] * 2 + df['B'] ** 2 - df['C'] / 3

# Numexpræ–¹å¼ï¼ˆå¿«3-5å€ï¼‰
df['result'] = ne.evaluate(
    "A * 2 + B ** 2 - C / 3",
    local_dict={k: df[k].values for k in ['A', 'B', 'C']}
)
```

### **4. é«˜çº§ä¼˜åŒ–æŠ€å·§ (åç«¯å·²å®‰è£…æ”¯æŒ)**

#### **å¼‚æ­¥æ–‡ä»¶æ“ä½œ - aiofiles**
```python
import aiofiles
import asyncio

async def process_large_file():
    # å¼‚æ­¥è¯»å–ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹ï¼ˆæœºæ¢°ç¡¬ç›˜ç‰¹åˆ«å—ç›Šï¼‰
    async with aiofiles.open('/data/large_file.csv', 'r') as f:
        content = await f.read()
    
    # å¤„ç†æ•°æ®...
    
    # å¼‚æ­¥å†™å…¥
    async with aiofiles.open('/data/processed.csv', 'w') as f:
        await f.write(processed_content)

# åœ¨å¼‚æ­¥ç¯å¢ƒä¸­è°ƒç”¨
await process_large_file()
```

#### **å†…å­˜ç¼“å­˜ä¸å¹¶è¡Œè®¡ç®— - joblib**
```python
from joblib import Memory
import time

# åˆ›å»ºå†…å­˜ç¼“å­˜ï¼ˆå¯é…ç½®åˆ°ç£ç›˜ï¼‰
cachedir = '/data/cache'
memory = Memory(cachedir, verbose=0)

@memory.cache
def expensive_computation(x, y):
    """è®¡ç®—ç»“æœä¼šè¢«ç¼“å­˜åˆ°ç£ç›˜"""
    time.sleep(2)  # æ¨¡æ‹Ÿè€—æ—¶è®¡ç®—
    return x * y + x**2

# ç¬¬ä¸€æ¬¡è®¡ç®—æ…¢ï¼Œåç»­ä»ç£ç›˜è¯»å–å¿«
result1 = expensive_computation(10, 20)  # æ…¢
result2 = expensive_computation(10, 20)  # å¿«ï¼ˆä»ç¼“å­˜ï¼‰
```

#### **DuckDBæ›¿ä»£Pandasé‡æ“ä½œ**
```python
import duckdb

# âŒ è€—å†…å­˜çš„Pandasæ“ä½œ
# df = pd.read_csv('/data/large.csv')
# result = df.groupby('category').agg({'value': ['mean', 'sum', 'count']})

# âœ… å†…å­˜å‹å¥½çš„DuckDBæ“ä½œ
result = duckdb.sql("""
    SELECT category, 
           AVG(value) as mean_value,
           SUM(value) as sum_value,
           COUNT(value) as count_value
    FROM read_csv('/data/large.csv')
    GROUP BY category
""").df()
```

---

## ğŸ“‹ **å¯ç”¨åº“å¿«é€Ÿå‚è€ƒ (ä¸Dockerfileå®Œå…¨ä¸€è‡´)**

### **æ•°æ®å¤„ç†æ ¸å¿ƒ**
```python
import pandas as pd          # æ•°æ®åˆ†æ (v2.2.2)
import numpy as np           # æ•°å€¼è®¡ç®— (v1.26.4)
import duckdb                # å†…å­˜SQL (v0.10.2)
import numexpr as ne         # è¡¨è¾¾å¼åŠ é€Ÿ (v2.10.0)
import bottleneck as bn      # æ»šåŠ¨ç»Ÿè®¡åŠ é€Ÿ (v1.3.8)
import pyarrow.feather as feather  # Featheræ ¼å¼æ”¯æŒ (v14.0.2)
import polars as pl          # é«˜æ€§èƒ½DataFrame (v0.20.3)
```

### **æœºå™¨å­¦ä¹ å¢å¼º**
```python
from sklearn.ensemble import RandomForestClassifier  # scikit-learn v1.5.0
import lightgbm as lgb       # æ¢¯åº¦æå‡æ ‘ (v4.3.0)
import category_encoders as ce  # åˆ†ç±»ç¼–ç  (v2.6.3)
from skopt import BayesSearchCV  # è´å¶æ–¯ä¼˜åŒ– (v0.9.0)
import statsmodels.api as sm  # ç»Ÿè®¡æ¨¡å‹ (v0.14.1)
```

### **å¯è§†åŒ–ä¸å›¾è¡¨**
```python
import matplotlib.pyplot as plt  # åŸºç¡€ç»˜å›¾ (v3.8.4)
import seaborn as sns            # ç»Ÿè®¡å¯è§†åŒ– (v0.13.2)
import graphviz                  # æµç¨‹å›¾ (è‡ªåŠ¨å¸ƒå±€) - ç³»ç»Ÿå®‰è£…
import networkx as nx            # ç½‘ç»œå›¾
```

### **æ–‡æ¡£ç”Ÿæˆ**
```python
from docx import Document        # Wordæ–‡æ¡£ (v1.1.2)
from reportlab.lib.pagesizes import letter  # PDFç”Ÿæˆ (v4.0.7)
from pptx import Presentation    # PPTæ¼”ç¤ºæ–‡ç¨¿ (v0.6.23)
import openpyxl                  # Excelæ“ä½œ (v3.1.2)
```

### **ç§‘å­¦è®¡ç®—ä¸æ•°å­¦**
```python
import sympy as sp               # ç¬¦å·æ•°å­¦ (v1.12)
import scipy                     # ç§‘å­¦è®¡ç®— (v1.14.1)
import scipy.optimize as opt     # ä¼˜åŒ–ç®—æ³•
```

### **ç½‘é¡µå†…å®¹å¤„ç†**
```python
from bs4 import BeautifulSoup    # HTMLè§£æ (v4.12.3)
import lxml                      # é«˜æ€§èƒ½è§£æå™¨ (v5.2.2)
from tabulate import tabulate    # æ ¼å¼åŒ–è¡¨æ ¼ (v0.9.0)
```

### **æ€§èƒ½ä¼˜åŒ–ä¸å·¥å…·**
```python
from tqdm import tqdm            # è¿›åº¦æ¡æ˜¾ç¤º (v4.66.4)
from joblib import Memory        # ç£ç›˜ç¼“å­˜å’Œå¹¶è¡Œ (v1.3.2)
import aiofiles                  # å¼‚æ­¥æ–‡ä»¶æ“ä½œ (v24.1.0)
```

### **åç«¯æ¡†æ¶ä¾èµ–**
```python
# ä»¥ä¸‹åº“å·²åœ¨åç«¯å®‰è£…ï¼Œä½†ç”¨æˆ·ä»£ç é€šå¸¸ä¸éœ€è¦ç›´æ¥ä½¿ç”¨
# fastapi, uvicorn, docker, pydot ç­‰
```

---

## ğŸš¨ **é‡è¦é™åˆ¶ä¸æœ€ä½³å®è·µ**

### **âœ… å¿…é¡»éµå®ˆçš„è§„åˆ™**
1. **å›¾è¡¨è¾“å‡º**: æ€»æ˜¯ä½¿ç”¨ `plt.show()`ï¼Œç³»ç»Ÿè‡ªåŠ¨æ•è·
2. **æ–‡ä»¶ç”Ÿæˆ**: å¿…é¡»è¾“å‡ºç‰¹å®šJSONæ ¼å¼ç»™å¯ä¸‹è½½æ–‡ä»¶
3. **æ–‡ä»¶è®¿é—®**: æ•°æ®æ–‡ä»¶åœ¨ `/data` ç›®å½•ï¼Œåª’ä½“æ–‡ä»¶åœ¨ä¸Šä¸‹æ–‡ä¸­
4. **å†…å­˜ç®¡ç†**: å®¹å™¨é™åˆ¶6GBï¼Œ**Swapå·²ç¦ç”¨**ï¼Œé¿å…ä½¿ç”¨swap
5. **ä¼šè¯ç®¡ç†**: ä½¿ç”¨session_idä¿æŒæ–‡ä»¶æŒä¹…æ€§
6. **ä»£ç ç»“æ„**: é¿å…ç±»å®šä¹‰ï¼Œä½¿ç”¨çº¯å‡½æ•°å¼ç¼–ç¨‹

### **âŒ ç¦æ­¢çš„æ“ä½œ**
```python
# ä»¥ä¸‹æ“ä½œä¼šè¢«é˜»æ­¢ï¼š
exec("å±é™©ä»£ç ")                 # âŒ åŠ¨æ€æ‰§è¡Œï¼ˆåç«¯é™åˆ¶exec_globalsï¼‰
__import__('os').system('rm')   # âŒ ç³»ç»Ÿå‘½ä»¤ï¼ˆç½‘ç»œç¦ç”¨ï¼‰
open('/etc/passwd')             # âŒ è®¿é—®ç³»ç»Ÿæ–‡ä»¶ï¼ˆæ ¹ç›®å½•åªè¯»ï¼‰
class MyClass:                   # âŒ ç±»å®šä¹‰ï¼ˆsandboxé™åˆ¶ï¼‰
    pass
# è®¿é—®ç½‘ç»œèµ„æº                    # âŒ ç½‘ç»œå®Œå…¨ç¦ç”¨
```

### **âš ï¸ æ€§èƒ½è­¦å‘Š**
1. **å¤§æ–‡ä»¶**: >50MBæ—¶ä½¿ç”¨åˆ†å—å¤„ç†
2. **å¤æ‚è®¡ç®—**: ä½¿ç”¨DuckDBæˆ–NumexpråŠ é€Ÿ
3. **é‡å¤æ“ä½œ**: ä½¿ç”¨Featheræ ¼å¼ç¼“å­˜ä¸­é—´ç»“æœ
4. **å†…å­˜ç›‘æ§**: åŠæ—¶åˆ é™¤å¤§å˜é‡ `del large_df`
5. **Swapå·²ç¦ç”¨**: å†…å­˜è¶…é™ç›´æ¥å´©æºƒï¼Œæ³¨æ„å†…å­˜ä½¿ç”¨

### **ğŸ”§ é«˜çº§ä½¿ç”¨å»ºè®®**
1. **çº¯å‡½æ•°å¼ç¼–ç¨‹**: ä½¿ç”¨å­—å…¸å’Œåˆ—è¡¨ç»„ç»‡æ•°æ®ï¼Œé¿å…ç±»å®šä¹‰
2. **å¤æ‚é€»è¾‘æ‹†åˆ†**: å°†å¤æ‚ä»»åŠ¡æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°
3. **åˆ†æ­¥éª¤æ‰§è¡Œ**: åˆ©ç”¨ä¼šè¯æŒä¹…åŒ–ï¼Œåˆ†æ­¥æ‰§è¡Œå¤æ‚åˆ†æ
4. **å­—ä½“æ”¯æŒ**: å·²å®‰è£…ä¸­æ–‡å­—ä½“ï¼ˆæ–‡æ³‰é©¿å¾®ç±³é»‘/æ­£é»‘ï¼‰ï¼Œå›¾è¡¨æ”¯æŒä¸­æ–‡

---

## ğŸ”§ **æ•…éšœæ’é™¤ä¸è°ƒè¯•**

### **å¸¸è§é—®é¢˜è§£å†³**

#### **é—®é¢˜1: å†…å­˜ä¸è¶³**
```python
# âŒ é”™è¯¯åšæ³•
df = pd.read_csv('/data/huge.csv')  # å¯èƒ½å´©æºƒ

# âœ… æ­£ç¡®åšæ³•
# æ–¹æ¡ˆA: åˆ†å—å¤„ç†
for chunk in pd.read_csv('/data/huge.csv', chunksize=50000):
    process(chunk)

# æ–¹æ¡ˆB: ä½¿ç”¨DuckDBå†…å­˜å¤–æŸ¥è¯¢
result = duckdb.sql("SELECT * FROM read_csv_auto('/data/huge.csv') LIMIT 10000").df()

# æ–¹æ¡ˆC: è½¬æ¢ä¸ºFeatheræ ¼å¼
import pyarrow.feather as feather
df = pd.read_csv('/data/huge.csv')
feather.write_feather(df, '/data/huge.feather')  # ä¿å­˜ä¸ºé«˜æ•ˆæ ¼å¼
df_fast = feather.read_feather('/data/huge.feather')  # å¿«é€Ÿè¯»å–
```

#### **é—®é¢˜2: å¤„ç†é€Ÿåº¦æ…¢**
```python
# âŒ æ…¢é€ŸPandasæ“ä½œ
df['result'] = df['A'] * 2 + df['B'] ** 2 - df['C'] / 3

# âœ… ä½¿ç”¨NumexpråŠ é€Ÿ
df['result'] = ne.evaluate("A * 2 + B ** 2 - C / 3", 
                          {k: df[k].values for k in ['A', 'B', 'C']})

# âœ… ä½¿ç”¨BottleneckåŠ é€Ÿæ»šåŠ¨ç»Ÿè®¡
import bottleneck as bn
df['rolling_mean'] = bn.move_mean(df['value'], window=20)
```

#### **é—®é¢˜3: å›¾è¡¨ä¸æ˜¾ç¤º**
```python
# âŒ ç¼ºå°‘show()
plt.plot(x, y)
plt.title('å›¾è¡¨')

# âœ… å¿…é¡»è°ƒç”¨show()
plt.plot(x, y)
plt.title('å›¾è¡¨')
plt.show()  # ğŸ¯ å…³é”®ï¼

# âœ… Graphvizå›¾è¡¨è‡ªåŠ¨æ•è·ï¼ˆæ— éœ€é¢å¤–è°ƒç”¨ï¼‰
dot = Digraph()
dot.node('A', 'å¼€å§‹')
# åˆ›å»ºå¯¹è±¡å³è‡ªåŠ¨æ•è·
```

#### **é—®é¢˜4: å¤§å‹æ–‡ä»¶IOæ…¢**
```python
# âŒ åŒæ­¥IOé˜»å¡
with open('/data/large.txt', 'r') as f:
    content = f.read()  # é˜»å¡ä¸»çº¿ç¨‹

# âœ… å¼‚æ­¥IO (æœºæ¢°ç¡¬ç›˜ç‰¹åˆ«æœ‰æ•ˆ)
import aiofiles
import asyncio

async def read_file_async():
    async with aiofiles.open('/data/large.txt', 'r') as f:
        return await f.read()
```

### **æ€§èƒ½ç›‘æ§å‘½ä»¤ (å®Œæ•´ç‰ˆè¡¥å……)**
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
watch -n 2 "free -h | grep -E 'Mem|Swap'"

# ç›‘æ§ç£ç›˜IOï¼ˆæœºæ¢°ç¡¬ç›˜å…³é”®æŒ‡æ ‡ï¼‰
iostat -x 2

# ç›‘æ§Dockerå®¹å™¨
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"
```

---

## ğŸ“ˆ **ç‰ˆæœ¬æ›´æ–°æ—¥å¿—**

### **v2.5 æ ¸å¿ƒå‡çº§ (å½“å‰ç‰ˆæœ¬)**
1. **æ€§èƒ½åº“æ–°å¢**: DuckDB (å†…å­˜SQL)ã€Numexpr (è¡¨è¾¾å¼åŠ é€Ÿ)ã€Bottleneck (æ»šåŠ¨ç»Ÿè®¡)
2. **MLå¢å¼º**: LightGBMã€Category Encodersã€scikit-optimize (è´å¶æ–¯ä¼˜åŒ–)
3. **å·¥å…·å®Œå–„**: tqdmè¿›åº¦æ¡ã€joblibç¼“å­˜ã€aiofileså¼‚æ­¥IO
4. **æœºæ¢°ç¡¬ç›˜ä¼˜åŒ–**: Swapç¦ç”¨é˜²æ­¢æ­»æœºï¼ŒFeatheræ ¼å¼æ”¯æŒ
5. **åº“ç‰ˆæœ¬å‡çº§**: 
   - scikit-learnå‡çº§åˆ°1.5.0
   - pandaså‡çº§åˆ°2.2.2
   - æ–°å¢polars-lts-cpu==0.20.3

### **v2.4 ä¸»è¦åŠŸèƒ½**
- æ–‡æœ¬åˆ†æèƒ½åŠ› (BeautifulSoup4 + lxml)
- å›¾è¡¨è‡ªåŠ¨æ•è·ç³»ç»Ÿå®Œå–„
- ä¼šè¯æ–‡ä»¶ç®¡ç†ä¼˜åŒ–

### **v2.3 åŠæ›´æ—©**
- åŸºç¡€æ²™ç›’åŠŸèƒ½
- å›¾è¡¨è‡ªåŠ¨æ•è·
- æ–‡ä»¶ä¸Šä¼ æ”¯æŒ

---

## ğŸ¯ **å¿«é€Ÿå¼€å§‹æ¨¡æ¿**

### **æ¨¡æ¿1: åŸºç¡€æ•°æ®åˆ†æ**
```python
import pandas as pd
import matplotlib.pyplot as plt

# 1. è¯»å–æ•°æ®
df = pd.read_csv('/data/data.csv')

# 2. å¿«é€Ÿåˆ†æ
print(f"æ•°æ®å½¢çŠ¶: {df.shape}")
print(df.describe())

# 3. ç®€å•å¯è§†åŒ–
df.groupby('category')['value'].mean().plot(kind='bar')
plt.title('å„åˆ†ç±»å¹³å‡å€¼')
plt.tight_layout()
plt.show()  # ğŸ¯ è‡ªåŠ¨æ•è·å›¾è¡¨
```

### **æ¨¡æ¿2: å®Œæ•´æŠ¥å‘Šç”Ÿæˆ**
```python
# å‚è€ƒ: report_generator_workflow.md
# åŒ…å«æ•°æ®è¯»å–ã€åˆ†æã€å›¾è¡¨ã€æ–‡æ¡£ç”Ÿæˆå…¨æµç¨‹

import pandas as pd
import matplotlib.pyplot as plt
from docx import Document
import base64, json

# 1. æ•°æ®è¯»å–ä¸åˆ†æ
df = pd.read_excel('/data/sales_data.xlsx')
summary = df.groupby('region')['sales'].sum()

# 2. åˆ›å»ºå›¾è¡¨
summary.plot(kind='bar')
plt.title('å„åœ°åŒºé”€å”®æ€»é¢')
plt.tight_layout()
plt.show()  # ğŸ¯ è‡ªåŠ¨æ•è·

# 3. ç”ŸæˆWordæŠ¥å‘Š
doc = Document()
doc.add_heading('é”€å”®åˆ†ææŠ¥å‘Š', 0)
doc.add_paragraph(f"æ€»é”€å”®é¢: ${df['sales'].sum():,.2f}")
doc.add_paragraph(f"å¹³å‡é”€å”®é¢: ${df['sales'].mean():,.2f}")

# 4. ä¿å­˜å¹¶è¾“å‡º
doc.save('/data/report.docx')
with open('/data/report.docx', 'rb') as f:
    file_data = base64.b64encode(f.read()).decode('utf-8')

# ğŸ¯ åç«¯å®é™…æ”¯æŒçš„è¾“å‡ºæ ¼å¼
output = {
    "type": "word",
    "title": "é”€å”®åˆ†ææŠ¥å‘Š.docx",
    "data_base64": file_data
}
print(json.dumps(output))
```

### **æ¨¡æ¿3: æœºå™¨å­¦ä¹ å»ºæ¨¡**
```python
# å‚è€ƒ: ml_workflow.md
# åŒ…å«æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# 1. åŠ è½½æ•°æ®
df = pd.read_csv('/data/iris.csv')

# 2. ç‰¹å¾ä¸æ ‡ç­¾
X = df.drop('species', axis=1)
y = df['species']

# 3. åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. è®­ç»ƒæ¨¡å‹
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 5. è¯„ä¼°
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
```

### **æ¨¡æ¿4: Graphvizæµç¨‹å›¾**
```python
from graphviz import Digraph

# åˆ›å»ºå·¥ä½œæµç¨‹å›¾
workflow = Digraph('å·¥ä½œæµç¨‹', format='png')
workflow.attr(rankdir='LR', size='8,5')

# æ·»åŠ èŠ‚ç‚¹
workflow.node('start', 'å¼€å§‹', shape='ellipse')
workflow.node('collect', 'æ•°æ®é‡‡é›†', shape='box')
workflow.node('clean', 'æ•°æ®æ¸…æ´—', shape='box')
workflow.node('analyze', 'æ•°æ®åˆ†æ', shape='box')
workflow.node('report', 'æŠ¥å‘Šç”Ÿæˆ', shape='box')
workflow.node('end', 'ç»“æŸ', shape='ellipse')

# æ·»åŠ è¾¹
workflow.edges([
    'startcollect', 'collectclean', 
    'cleananalyze', 'analyzereport', 'reportend'
])

# ğŸ¯ è‡ªåŠ¨æ•è·ï¼šGraphvizå¯¹è±¡åˆ›å»ºåè‡ªåŠ¨æ¸²æŸ“ä¸ºå›¾ç‰‡
# æ— éœ€è°ƒç”¨render()æˆ–show()
```

---

## ğŸ’¡ **ç»ˆææç¤º**

1. **ä¼˜å…ˆæŸ¥é˜…å‚è€ƒæ–‡ä»¶** - ä¸è¦é‡æ–°å‘æ˜è½®å­
2. **åˆ©ç”¨ä¼šè¯æŒä¹…åŒ–** - ä½¿ç”¨session_idä¿å­˜ä¸­é—´ç»“æœï¼Œåˆ†æ­¥æ‰§è¡Œå¤æ‚ä»»åŠ¡
3. **ä¿¡ä»»è‡ªåŠ¨åŒ–ç³»ç»Ÿ** - å›¾è¡¨ã€è¾“å‡ºæ ¼å¼ç­‰äº¤ç»™åç«¯å¤„ç†
4. **æ€§èƒ½æ•æ„Ÿç”¨ä¼˜åŒ–åº“** - å¤§æ–‡ä»¶ç”¨DuckDBï¼Œå¤æ‚è®¡ç®—ç”¨Numexpr
5. **æµ‹è¯•ä»£ç ç‰‡æ®µ** - å¤æ‚é€»è¾‘å…ˆå°è§„æ¨¡æµ‹è¯•
6. **æ³¨æ„å†…å­˜é™åˆ¶** - Swapå·²ç¦ç”¨ï¼Œå†…å­˜è¶…é™ç›´æ¥å´©æºƒ
7. **ä½¿ç”¨æ­£ç¡®è¾“å‡ºæ ¼å¼** - åªä½¿ç”¨åç«¯æ”¯æŒçš„JSONè¾“å‡ºç±»å‹

---

## ğŸ”— **ç›¸å…³èµ„æº**

- **å®Œæ•´ç¤ºä¾‹åº“**: æ‰€æœ‰å‚è€ƒæ–‡ä»¶ä¸­çš„ä»£ç ç¤ºä¾‹
- **æ€§èƒ½æµ‹è¯•**: å¯¹æ¯”ä¸åŒæ–¹æ³•çš„æ‰§è¡Œæ•ˆç‡
- **æœ€ä½³å®è·µ**: å„é¢†åŸŸçš„æ ‡å‡†åŒ–å·¥ä½œæµ
- **æ•…éšœæ¡ˆä¾‹**: å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

**è®°ä½**: è¿™ä¸ªæ²™ç›’ç¯å¢ƒå·²ç»é¢„é…ç½®äº†æ‰€æœ‰åº“å’Œä¼˜åŒ–ï¼Œä½ åªéœ€è¦ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘ï¼ç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æŠ€æœ¯ç»†èŠ‚ï¼Œè®©ä½ åƒåœ¨æœ¬åœ°ç¯å¢ƒä¸€æ ·é¡ºç•…å·¥ä½œã€‚
