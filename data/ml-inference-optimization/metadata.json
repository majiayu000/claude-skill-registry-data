{
  "name": "ml-inference-optimization",
  "description": "ML inference latency optimization, model compression, distillation, caching strategies, and edge deployment patterns. Use when optimizing inference performance, reducing model size, or deploying ML at",
  "repo": "melodic-software/claude-code-plugins",
  "category": "data",
  "tags": [],
  "stars": 0,
  "source": "SkillsMP + GitHub Raw",
  "dir_name": "ml-inference-optimization"
}