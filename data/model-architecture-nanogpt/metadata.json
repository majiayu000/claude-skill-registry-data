{
  "name": "model-architecture-nanogpt",
  "description": "Educational GPT implementation in ~300 lines. Reproduces GPT-2 (124M) on OpenWebText. Clean, hackable code for learning transformers. By Andrej Karpathy. Perfect for understanding GPT architecture fro",
  "repo": "davila7/claude-code-templates",
  "category": "other",
  "tags": [],
  "stars": 0,
  "source": "github.com/davila7/claude-code-templates"
}