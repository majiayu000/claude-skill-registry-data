{
  "name": "training-optimization",
  "repo": "ScientiaCapital/unsloth-mcp-server",
  "category": "data",
  "description": "Advanced techniques for optimizing LLM fine-tuning. Covers learning rates, LoRA configuration, batch sizes, gradient strategies, hyperparameter tuning, and monitoring. Use when fine-tuning models for ",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T09:03:23.536663Z",
  "dir_name": "training-optimization",
  "github_path": ".claude/skills/training-optimization",
  "github_branch": "main"
}