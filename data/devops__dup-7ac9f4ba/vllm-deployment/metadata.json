{
  "name": "vllm-deployment",
  "description": "Deploy vLLM for high-performance LLM inference. Covers Docker CPU/GPU deployments and cloud VM provisioning with OpenAI-compatible API endpoints.",
  "repo": "stakpak/community-paks",
  "category": "devops",
  "tags": [
    "vllm",
    "deployment"
  ],
  "stars": 3,
  "github_path": "vllm-deployment",
  "github_branch": "main"
}