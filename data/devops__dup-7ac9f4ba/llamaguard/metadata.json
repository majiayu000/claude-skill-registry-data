{
  "name": "llamaguard",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Meta's 7-8B specialized moderation model for LLM input/output filtering. 6 safety categories - violence/hate, sexual content, weapons, substances, self-harm, criminal planning. 94-95% accuracy. Deploy",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:18.647102Z",
  "github_path": "07-safety-alignment/llamaguard",
  "github_branch": "main"
}