---
name: slo-sli-design
description: Design Service Level Objectives, Indicators, and error budgets
allowed-tools: Read, Glob, Grep, Write, Edit
---

# SLO/SLI Design Skill

## When to Use This Skill

Use this skill when:

- **Slo Sli Design tasks** - Working on design service level objectives, indicators, and error budgets
- **Planning or design** - Need guidance on Slo Sli Design approaches
- **Best practices** - Want to follow established patterns and standards

## Overview

Design Service Level Objectives, Indicators, and error budget policies.

## MANDATORY: Documentation-First Approach

Before designing SLOs:

1. **Invoke `docs-management` skill** for SLO/SLI patterns
2. **Verify SRE practices** via MCP servers (perplexity)
3. **Base guidance on Google SRE and industry best practices**

## SLO/SLI/SLA Hierarchy

```text
SLO/SLI/SLA RELATIONSHIP:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚  SLA (Service Level Agreement)                                   â”‚
â”‚  â”œâ”€â”€ External promise to customers                               â”‚
â”‚  â”œâ”€â”€ Legal/contractual implications                              â”‚
â”‚  â””â”€â”€ Example: "99.9% monthly uptime"                             â”‚
â”‚                                                                  â”‚
â”‚       â–²                                                          â”‚
â”‚       â”‚ Buffer (SLO should be tighter)                           â”‚
â”‚       â”‚                                                          â”‚
â”‚  SLO (Service Level Objective)                                   â”‚
â”‚  â”œâ”€â”€ Internal reliability target                                 â”‚
â”‚  â”œâ”€â”€ Tighter than SLA (headroom)                                 â”‚
â”‚  â””â”€â”€ Example: "99.95% monthly availability"                      â”‚
â”‚                                                                  â”‚
â”‚       â–²                                                          â”‚
â”‚       â”‚ Measured by                                              â”‚
â”‚       â”‚                                                          â”‚
â”‚  SLI (Service Level Indicator)                                   â”‚
â”‚  â”œâ”€â”€ Actual measurement                                          â”‚
â”‚  â”œâ”€â”€ Quantitative metric                                         â”‚
â”‚  â””â”€â”€ Example: "successful_requests / total_requests"             â”‚
â”‚                                                                  â”‚
â”‚       â–²                                                          â”‚
â”‚       â”‚ Derived from                                             â”‚
â”‚       â”‚                                                          â”‚
â”‚  Error Budget                                                    â”‚
â”‚  â”œâ”€â”€ Allowable unreliability: 100% - SLO                         â”‚
â”‚  â”œâ”€â”€ Example: 0.05% = 21.6 minutes/month                         â”‚
â”‚  â””â”€â”€ Spent on: releases, incidents, maintenance                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Common SLI Types

```text
SLI CATEGORIES:

AVAILABILITY SLI:
"The proportion of requests that are served successfully"

Formula: successful_requests / total_requests Ã— 100%

Good Events: HTTP 2xx, 3xx, 4xx (client errors)
Bad Events: HTTP 5xx, timeouts, connection failures

Example Prometheus query:
  sum(rate(http_requests_total{status!~"5.."}[5m]))
  /
  sum(rate(http_requests_total[5m]))

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LATENCY SLI:
"The proportion of requests that are served within threshold"

Formula: requests_below_threshold / total_requests Ã— 100%

Thresholds (example):
- P50: 100ms (median experience)
- P95: 500ms (95th percentile)
- P99: 1000ms (tail latency)

Example Prometheus query:
  sum(rate(http_request_duration_bucket{le="0.5"}[5m]))
  /
  sum(rate(http_request_duration_count[5m]))

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

QUALITY/CORRECTNESS SLI:
"The proportion of requests that return correct results"

Formula: correct_responses / total_responses Ã— 100%

Good Events: Valid data, expected format
Bad Events: Data corruption, stale data, wrong results

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

FRESHNESS SLI:
"The proportion of data that is updated within threshold"

Formula: fresh_records / total_records Ã— 100%

Example: "95% of records updated within 5 minutes"

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

THROUGHPUT SLI:
"The proportion of time system handles expected load"

Formula: time_at_capacity / total_time Ã— 100%

Example: "System handles 1000 req/s 99% of the time"
```

## Error Budget Calculation

```text
ERROR BUDGET MATH:

Monthly Error Budget (30 days):

SLO Target  â”‚ Error Budget â”‚ Allowed Downtime
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
99%         â”‚ 1%           â”‚ 7h 18m
99.5%       â”‚ 0.5%         â”‚ 3h 39m
99.9%       â”‚ 0.1%         â”‚ 43m 50s
99.95%      â”‚ 0.05%        â”‚ 21m 55s
99.99%      â”‚ 0.01%        â”‚ 4m 23s
99.999%     â”‚ 0.001%       â”‚ 26s

Error Budget Consumption:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                  â”‚
â”‚  Monthly Budget: 21m 55s (99.95% SLO)                            â”‚
â”‚                                                                  â”‚
â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  Used: 8m (36%)               â”‚
â”‚                                                                  â”‚
â”‚  Incidents:                                                      â”‚
â”‚  - Jan 5: Database failover - 5m                                 â”‚
â”‚  - Jan 12: Deployment rollback - 3m                              â”‚
â”‚                                                                  â”‚
â”‚  Remaining: 13m 55s (64%)                                        â”‚
â”‚                                                                  â”‚
â”‚  Status: âœ“ HEALTHY                                               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SLO Design Process

```text
SLO DESIGN WORKFLOW:

Step 1: IDENTIFY USER JOURNEYS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What do users care about?                                        â”‚
â”‚                                                                  â”‚
â”‚ Critical User Journeys (CUJs):                                   â”‚
â”‚ - Login and authentication                                       â”‚
â”‚ - Search and browse products                                     â”‚
â”‚ - Add to cart and checkout                                       â”‚
â”‚ - View order status                                              â”‚
â”‚                                                                  â”‚
â”‚ For each journey:                                                â”‚
â”‚ - What constitutes success?                                      â”‚
â”‚ - What latency is acceptable?                                    â”‚
â”‚ - What's the business impact of failure?                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: DEFINE SLIs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What can we measure that represents user happiness?              â”‚
â”‚                                                                  â”‚
â”‚ For "Checkout" journey:                                          â”‚
â”‚ - Availability: checkout completes without error                 â”‚
â”‚ - Latency: checkout completes within 3 seconds                   â”‚
â”‚ - Correctness: order total matches cart                          â”‚
â”‚                                                                  â”‚
â”‚ SLI Specification:                                               â”‚
â”‚ - What events are we measuring?                                  â”‚
â”‚ - What's a "good" event vs "bad" event?                          â”‚
â”‚ - Where do we measure? (server, client, synthetic)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 3: SET SLO TARGETS
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What reliability level should we target?                         â”‚
â”‚                                                                  â”‚
â”‚ Consider:                                                        â”‚
â”‚ - Current baseline (what are we achieving now?)                  â”‚
â”‚ - User expectations (what do users tolerate?)                    â”‚
â”‚ - Business requirements (any SLAs?)                              â”‚
â”‚ - Cost vs reliability trade-off                                  â”‚
â”‚                                                                  â”‚
â”‚ Start achievable, improve iteratively                            â”‚
â”‚ SLO = Current baseline - small margin                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 4: DEFINE ERROR BUDGET POLICY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ What happens when budget is exhausted?                           â”‚
â”‚                                                                  â”‚
â”‚ Error Budget Policy:                                             â”‚
â”‚ - Budget > 50%: Normal operations                                â”‚
â”‚ - Budget 25-50%: Slow down risky changes                         â”‚
â”‚ - Budget < 25%: Focus on reliability                             â”‚
â”‚ - Budget = 0%: Feature freeze, reliability only                  â”‚
â”‚                                                                  â”‚
â”‚ Escalation:                                                      â”‚
â”‚ - Who gets notified at each threshold?                           â”‚
â”‚ - What actions are required?                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SLO Document Template

```markdown
# SLO: {Service Name} - {Journey/Feature}

## Service Overview

| Attribute | Value |
|-----------|-------|
| Service | [Service name] |
| Owner | [Team name] |
| Criticality | [Critical/High/Medium/Low] |
| User Journey | [Journey name] |

## SLI Specification

### Availability SLI

**Definition:** The proportion of [event type] that [success criteria].

**Good Event:** [What counts as success]
**Bad Event:** [What counts as failure]

**Measurement:**
- Source: [Prometheus/Azure Monitor/etc.]
- Query:
  ```promql
  sum(rate(http_requests_total{status!~"5.."}[5m]))
  /
  sum(rate(http_requests_total[5m]))
  ```

### Latency SLI

**Definition:** The proportion of requests served within [threshold].

**Thresholds:**

| Percentile | Threshold |
|------------|-----------|
| P50 | [X]ms |
| P95 | [X]ms |
| P99 | [X]ms |

**Measurement:**

```promql
histogram_quantile(0.95,
  rate(http_request_duration_bucket[5m]))
```

## SLO Targets

| SLI | Target | Window |
|-----|--------|--------|
| Availability | [99.9%] | 30 days rolling |
| Latency (P95) | [99%] below 500ms | 30 days rolling |

## Error Budget

| SLO | Error Budget | Allowed Downtime (30d) |
|-----|--------------|------------------------|
| 99.9% availability | 0.1% | 43m 50s |
| 99% latency | 1% | 7h 18m |

## Error Budget Policy

### Budget Thresholds

| Budget Remaining | Status | Actions |
|------------------|--------|---------|
| > 50% | ðŸŸ¢ Healthy | Normal operations |
| 25-50% | ðŸŸ¡ Caution | Review recent changes |
| 10-25% | ðŸŸ  Warning | Slow deployments, reliability focus |
| < 10% | ðŸ”´ Critical | Feature freeze |
| Exhausted | â›” Frozen | Reliability-only work |

### Escalation

| Threshold | Notify | Action Required |
|-----------|--------|-----------------|
| < 50% | Team lead | Awareness |
| < 25% | Engineering manager | Review deployment pace |
| < 10% | Director | Feature freeze decision |
| Exhausted | VP Engineering | Incident response mode |

## Alerting

### SLO Burn Rate Alerts

| Severity | Burn Rate | Time Window | Example |
|----------|-----------|-------------|---------|
| Critical | 14.4x | 1h | Budget exhausted in ~2 days |
| Warning | 6x | 6h | Budget exhausted in ~5 days |
| Info | 1x | 3d | Budget on track to exhaust |

### Alert Configuration

```yaml
- alert: SLOHighBurnRate
  expr: |
    (
      sum(rate(http_requests_total{status=~"5.."}[1h]))
      /
      sum(rate(http_requests_total[1h]))
    ) > (14.4 * 0.001)  # 14.4x burn rate for 99.9% SLO
  for: 2m
  labels:
    severity: critical
  annotations:
    summary: "High error budget burn rate"
    description: "Error budget burning at 14.4x rate"
```

## Review Schedule

- **Weekly:** SLO dashboard review
- **Monthly:** Error budget retrospective
- **Quarterly:** SLO target review

## Appendix: Historical Performance

[Include baseline measurements and trends]

```text

```

## .NET SLO Implementation

```csharp
// SLO metric implementation in .NET
// Infrastructure/Telemetry/SloMetrics.cs

using System.Diagnostics.Metrics;

public class SloMetrics
{
    private readonly Counter<long> _totalRequests;
    private readonly Counter<long> _successfulRequests;
    private readonly Counter<long> _failedRequests;
    private readonly Histogram<double> _requestDuration;

    public SloMetrics(IMeterFactory meterFactory)
    {
        var meter = meterFactory.Create("OrdersApi.SLO");

        _totalRequests = meter.CreateCounter<long>(
            "slo.requests.total",
            "{request}",
            "Total requests for SLO calculation");

        _successfulRequests = meter.CreateCounter<long>(
            "slo.requests.successful",
            "{request}",
            "Successful requests (good events)");

        _failedRequests = meter.CreateCounter<long>(
            "slo.requests.failed",
            "{request}",
            "Failed requests (bad events)");

        _requestDuration = meter.CreateHistogram<double>(
            "slo.request.duration",
            "ms",
            "Request duration for latency SLI");
    }

    public void RecordRequest(
        string endpoint,
        int statusCode,
        double durationMs)
    {
        var tags = new TagList
        {
            { "endpoint", endpoint },
            { "status_code", statusCode.ToString() }
        };

        _totalRequests.Add(1, tags);

        // Availability SLI: 5xx = bad, everything else = good
        if (statusCode >= 500)
        {
            _failedRequests.Add(1, tags);
        }
        else
        {
            _successfulRequests.Add(1, tags);
        }

        // Latency SLI
        _requestDuration.Record(durationMs, tags);
    }
}

// Middleware to capture SLO metrics
public class SloMetricsMiddleware
{
    private readonly RequestDelegate _next;
    private readonly SloMetrics _sloMetrics;

    public SloMetricsMiddleware(RequestDelegate next, SloMetrics sloMetrics)
    {
        _next = next;
        _sloMetrics = sloMetrics;
    }

    public async Task InvokeAsync(HttpContext context)
    {
        var stopwatch = Stopwatch.StartNew();

        try
        {
            await _next(context);
        }
        finally
        {
            stopwatch.Stop();

            var endpoint = context.GetEndpoint()?.DisplayName ?? "unknown";
            var statusCode = context.Response.StatusCode;
            var durationMs = stopwatch.Elapsed.TotalMilliseconds;

            _sloMetrics.RecordRequest(endpoint, statusCode, durationMs);
        }
    }
}
```

## Error Budget Dashboard Queries

```promql
# Availability SLI (30-day rolling)
1 - (
  sum(increase(slo_requests_failed_total[30d]))
  /
  sum(increase(slo_requests_total[30d]))
)

# Latency SLI (P95 < 500ms, 30-day)
sum(increase(slo_request_duration_bucket{le="500"}[30d]))
/
sum(increase(slo_request_duration_count[30d]))

# Error Budget Remaining (availability)
1 - (
  (1 - 0.999)  # SLO target (99.9%)
  -
  (1 - (
    sum(increase(slo_requests_failed_total[30d]))
    /
    sum(increase(slo_requests_total[30d]))
  ))
) / (1 - 0.999)

# Error Budget Burn Rate (1h)
(
  sum(rate(slo_requests_failed_total[1h]))
  /
  sum(rate(slo_requests_total[1h]))
) / (1 - 0.999)  # Divide by error budget (0.1%)
```

## Workflow

When designing SLOs:

1. **Identify User Journeys**: What do users care about?
2. **Define SLIs**: What can we measure?
3. **Measure Baseline**: What are we achieving now?
4. **Set SLO Targets**: Achievable but aspirational
5. **Define Error Budget Policy**: What happens when budget is low?
6. **Implement Alerting**: Multi-window burn rate alerts
7. **Create Dashboards**: Visibility into SLO status
8. **Review Regularly**: Adjust based on learning

## References

For detailed guidance:

---

**Last Updated:** 2025-12-26
