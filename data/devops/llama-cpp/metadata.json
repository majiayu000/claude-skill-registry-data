{
  "name": "llama-cpp",
  "repo": "zechenzhangAGI/AI-research-SKILLs",
  "category": "devops",
  "description": "Runs LLM inference on CPU, Apple Silicon, and consumer GPUs without NVIDIA hardware. Use for edge deployment, M1/M2/M3 Macs, AMD/Intel GPUs, or when CUDA is unavailable. Supports GGUF quantization (1.",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:56:19.171777Z",
  "github_path": "12-inference-serving/llama-cpp",
  "github_branch": "main"
}