{
  "name": "vllm-deployment",
  "description": "Deploy vLLM for high-performance LLM inference. Covers Docker CPU/GPU deployments and cloud VM provisioning with OpenAI-compatible API endpoints.",
  "repo": "stakpak/community-paks",
  "category": "data",
  "tags": [
    "vllm",
    "deployment"
  ],
  "stars": 3,
  "source": "local",
  "dir_name": "vllm-deployment"
}