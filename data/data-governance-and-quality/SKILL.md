---
name: data-governance-and-quality
description: Data governance strategy, quality validation rules, and data dictionary management for vehicle insurance platform. Use when defining data quality standards, implementing validation rules, managing field mappings, resolving data conflicts, or establishing data governance processes. Covers data cleaning standards, quality metrics, and mapping management.
allowed-tools: Read, Edit, Grep, Glob
---

# Data Governance and Quality Management

Comprehensive data governance framework for vehicle insurance daily report analysis platform, covering data quality rules, field definitions, cleaning standards, mapping management, and audit mechanisms.

## When to Use This Skill

Activate this skill when you need to:
- Define or validate data quality rules
- Implement field validation logic
- Manage staff-institution mapping updates
- Resolve data conflicts or inconsistencies
- Establish data cleaning standards
- Create data quality reports
- Design data audit mechanisms
- Document business field definitions

## ğŸ“š ä¸€ã€æ•°æ®å­—å…¸ (Data Dictionary)

### 1.1 æ ¸å¿ƒä¸šåŠ¡å­—æ®µå®šä¹‰

**å…³é”®æ–‡ä»¶**: [docs/FIELD_MAPPING.md](../../../docs/FIELD_MAPPING.md)

#### ç»´åº¦å­—æ®µ (Dimensions)

| å­—æ®µå | æ•°æ®ç±»å‹ | ä¸šåŠ¡å«ä¹‰ | æšä¸¾å€¼ | å¿…å¡« |
|--------|----------|----------|--------|------|
| ä¸‰çº§æœºæ„ | String | ä¸šåŠ¡å‘˜æ‰€å±ä¸‰çº§æœºæ„ | è¾¾å·/å¾·é˜³/ç»µé˜³/å—å……ç­‰ | âœ… |
| å››çº§æœºæ„ | String | ä¸šåŠ¡å‘˜æ‰€å±å››çº§æœºæ„ | è¾¾å·/å¾·é˜³ç­‰ | âŒ |
| å›¢é˜Ÿç®€ç§° | String | ä¸šåŠ¡å‘˜æ‰€å±å›¢é˜Ÿ | è¾¾å·ä¸šåŠ¡ä¸€éƒ¨ç­‰ | âŒ |
| ä¸šåŠ¡å‘˜ | String | é”€å”®äººå‘˜å§“å | ä»æ˜ å°„è¡¨æå– | âœ… |
| å®¢æˆ·ç±»åˆ«3 | String | å®¢æˆ·ç±»å‹ | éè¥ä¸šä¸ªäººå®¢è½¦/æ‘©æ‰˜è½¦ç­‰9ç±» | âœ… |
| é™©ç§å¤§ç±» | String | ä¿é™©äº§å“ç±»åˆ« | è½¦é™©/å…¶ä»– | âœ… |
| é™©ç§åç§° | String | å…·ä½“é™©ç§ | 0312/0313/0317ç­‰ | âœ… |
| æ˜¯å¦ç»­ä¿ | String | ç»­ä¿çŠ¶æ€ | æ–°ä¿/ç»­ä¿/è½¬ä¿ | âœ… |
| æ˜¯å¦æ–°èƒ½æº | String | æ–°èƒ½æºæ ‡å¿— | æ˜¯/å¦ | âœ… |
| æ˜¯å¦è¿‡æˆ·è½¦ | String | è¿‡æˆ·è½¦æ ‡å¿— | æ˜¯/å¦ | âœ… |
| ç»ˆç«¯æ¥æº | String | é”€å”®æ¸ é“ | 0110èåˆé”€å”®ç­‰ | âœ… |
| å¨ä½åˆ†æ®µ | String | è´§è½¦å¨ä½åŒºé—´ | <2å¨/2-5å¨/5-10å¨/>10å¨ | âŒ |

#### åº¦é‡å­—æ®µ (Metrics)

| å­—æ®µå | æ•°æ®ç±»å‹ | ä¸šåŠ¡å«ä¹‰ | èŒƒå›´ | å¿…å¡« | ç‰¹æ®Šè§„åˆ™ |
|--------|----------|----------|------|------|----------|
| ç­¾å•/æ‰¹æ”¹ä¿è´¹ | Numeric | ç­¾å•å‡€ä¿è´¹ | å¯è´Ÿæ•° | âœ… | å…è®¸è´Ÿå€¼(é€€ä¿/è°ƒæ•´) |
| ç­¾å•æ•°é‡ | Numeric | ä¿å•æ•°é‡ | â‰¥1 | âœ… | æ•´æ•° |
| æ‰‹ç»­è´¹å«ç¨ | Numeric | å«ç¨æ‰‹ç»­è´¹ | â‰¥0 | âŒ | å…è®¸ä¸º0 |
| ç­¾å•/æ‰¹æ”¹ä¿é¢ | Numeric | ä¿é¢ | >0 | âŒ | è´Ÿæ•°ä¸ºå¼‚å¸¸ |
| æ‰‹ç»­è´¹æ¯”ä¾‹ | Numeric | æ‰‹ç»­è´¹å ä¿è´¹æ¯”ä¾‹ | 0-1 | âŒ | ç™¾åˆ†æ¯” |

#### æ—¶é—´å­—æ®µ (Temporal)

| å­—æ®µå | æ•°æ®ç±»å‹ | æ ¼å¼ | ä¸šåŠ¡å«ä¹‰ | å¿…å¡« |
|--------|----------|------|----------|------|
| æŠ•ä¿ç¡®è®¤æ—¶é—´ | Datetime | YYYY-MM-DD HH:MM:SS | ä¸šåŠ¡å£å¾„æ—¶é—´åŸºå‡† | âœ… |
| åˆ·æ–°æ—¶é—´ | Datetime | YYYY-MM-DD HH:MM:SS | æ•°æ®æœ€æ–°æ€§æ ‡å¿— | âœ… |
| ä¿é™©èµ·æœŸ | Datetime | YYYY-MM-DD | ä¿å•ç”Ÿæ•ˆæ—¥æœŸ | âœ… |

#### æ ‡è¯†å­—æ®µ (Identifiers)

| å­—æ®µå | æ•°æ®ç±»å‹ | ä¸šåŠ¡å«ä¹‰ | å”¯ä¸€æ€§ | å¿…å¡« | è„±æ• |
|--------|----------|----------|--------|------|------|
| ä¿å•å· | String | ä¿å•å”¯ä¸€æ ‡è¯† | âŒ (ç»“åˆæ—¶é—´) | âœ… | âŒ |
| è½¦ç‰Œå·ç  | String | è½¦è¾†æ ‡è¯† | âŒ | âŒ | âœ… |
| è¢«ä¿é™©äºº | String | å®¢æˆ·å§“å | âŒ | âœ… | âœ… |
| è½¦æ¶å· | String | è½¦è¾†VINç  | âœ… | âŒ | âœ… |

### 1.2 æ´¾ç”Ÿå­—æ®µ (Derived Fields)

**å®ç°ä½ç½®**: [backend/data_processor.py](../../../backend/data_processor.py)

| æ´¾ç”Ÿå­—æ®µ | æºå­—æ®µ | æ´¾ç”Ÿè§„åˆ™ | ç”¨é€” |
|----------|--------|----------|------|
| telesales_flag | ç»ˆç«¯æ¥æº | `== '0110èåˆé”€å”®'` | ç”µé”€ä¸šåŠ¡æ ‡è¯† |
| commercial_flag | é™©ç§åç§° | `âˆˆ {'0312','0313','0317'}` | å•†ä¸šé™©æ ‡è¯† |
| new_energy_flag | æ˜¯å¦æ–°èƒ½æº | `== 'æ˜¯'` | æ–°èƒ½æºè½¦ç­›é€‰ |
| transfer_flag | æ˜¯å¦è¿‡æˆ·è½¦ | `== 'æ˜¯'` | è¿‡æˆ·è½¦ç­›é€‰ |
| non_local_flag | æ˜¯å¦å¼‚åœ°è½¦ | `== 'æ˜¯'` | å¼‚åœ°è½¦ç­›é€‰ |
| premium_sum | ç­¾å•/æ‰¹æ”¹ä¿è´¹ | `SUM()` | ä¿è´¹æ±‡æ€» |
| policy_count | ç­¾å•æ•°é‡ | `COUNT()` where ä¿è´¹â‰¥50 | æœ‰æ•ˆä¿å•æ•° |

---

## ğŸ“‹ äºŒã€æ•°æ®è´¨é‡è§„åˆ™ (Data Quality Rules)

### 2.1 å¿…å¡«å­—æ®µæ ¡éªŒ (Required Fields Validation)

**ä¼˜å…ˆçº§**: P0 (é˜»æ–­æ€§)

```python
# å®ç°ç¤ºä¾‹ (ä¼ªä»£ç å‚è€ƒ)
REQUIRED_FIELDS = {
    'æŠ•ä¿ç¡®è®¤æ—¶é—´': 'Datetime',
    'ä¸‰çº§æœºæ„': 'String',
    'ä¸šåŠ¡å‘˜': 'String',
    'å®¢æˆ·ç±»åˆ«3': 'String',
    'ç­¾å•/æ‰¹æ”¹ä¿è´¹': 'Numeric',
    'ç­¾å•æ•°é‡': 'Numeric',
    'æ˜¯å¦ç»­ä¿': 'String'
}

def validate_required_fields(df):
    """
    å¿…å¡«å­—æ®µæ ¡éªŒ

    Returns:
        dict: {
            'valid': bool,
            'missing_fields': list,
            'missing_rows': int,
            'message': str
        }
    """
    missing_fields = []
    for field in REQUIRED_FIELDS.keys():
        if field not in df.columns:
            missing_fields.append(field)

    if missing_fields:
        return {
            'valid': False,
            'missing_fields': missing_fields,
            'missing_rows': 0,
            'message': f'ç¼ºå¤±å¿…å¡«å­—æ®µ: {", ".join(missing_fields)}'
        }

    # æ£€æŸ¥ç©ºå€¼è¡Œæ•°
    null_count = df[REQUIRED_FIELDS.keys()].isnull().any(axis=1).sum()

    return {
        'valid': null_count == 0,
        'missing_fields': [],
        'missing_rows': null_count,
        'message': f'å‘ç°{null_count}è¡Œæ•°æ®ç¼ºå¤±å¿…å¡«å­—æ®µå€¼' if null_count > 0 else 'æ‰€æœ‰å¿…å¡«å­—æ®µæ ¡éªŒé€šè¿‡'
    }
```

### 2.2 æ ¼å¼éªŒè¯ (Format Validation)

**ä¼˜å…ˆçº§**: P0 (é˜»æ–­æ€§)

#### æ—¥æœŸæ ¼å¼æ ¡éªŒ

```python
def validate_date_format(df, date_columns=['æŠ•ä¿ç¡®è®¤æ—¶é—´', 'åˆ·æ–°æ—¶é—´', 'ä¿é™©èµ·æœŸ']):
    """
    æ—¥æœŸæ ¼å¼æ ¡éªŒ

    è§„åˆ™:
    - æ ¼å¼: YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM:SS
    - æ—¶é—´èŒƒå›´: 2020-01-01 è‡³ å½“å‰æ—¥æœŸ+30å¤©
    """
    errors = []

    for col in date_columns:
        if col not in df.columns:
            continue

        # å°è¯•è½¬æ¢
        df[col] = pd.to_datetime(df[col], errors='coerce')

        # æ£€æŸ¥æ— æ•ˆæ—¥æœŸ
        invalid_count = df[col].isnull().sum()
        if invalid_count > 0:
            errors.append(f'{col}: {invalid_count}è¡Œæ—¥æœŸæ ¼å¼æ— æ•ˆ')

        # æ£€æŸ¥æ—¶é—´èŒƒå›´
        valid_data = df[col].dropna()
        min_date = pd.to_datetime('2020-01-01')
        max_date = pd.to_datetime('today') + pd.Timedelta(days=30)

        out_of_range = valid_data[(valid_data < min_date) | (valid_data > max_date)]
        if len(out_of_range) > 0:
            errors.append(f'{col}: {len(out_of_range)}è¡Œæ—¥æœŸè¶…å‡ºåˆç†èŒƒå›´')

    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'message': '; '.join(errors) if errors else 'æ—¥æœŸæ ¼å¼æ ¡éªŒé€šè¿‡'
    }
```

#### æ•°å€¼ç±»å‹æ ¡éªŒ

```python
def validate_numeric_types(df):
    """
    æ•°å€¼ç±»å‹æ ¡éªŒ

    è§„åˆ™:
    - ç­¾å•/æ‰¹æ”¹ä¿è´¹: æ•°å€¼å‹,å¯è´Ÿæ•°,èŒƒå›´[-1000000, 100000]
    - ç­¾å•æ•°é‡: æ­£æ•´æ•°,èŒƒå›´[1, 10000]
    - æ‰‹ç»­è´¹å«ç¨: éè´Ÿæ•°,èŒƒå›´[0, 50000]
    """
    errors = []

    # ä¿è´¹æ ¡éªŒ
    if 'ç­¾å•/æ‰¹æ”¹ä¿è´¹' in df.columns:
        df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'] = pd.to_numeric(df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'], errors='coerce')
        invalid = df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'].isnull().sum()
        if invalid > 0:
            errors.append(f'ç­¾å•/æ‰¹æ”¹ä¿è´¹: {invalid}è¡Œæ— æ³•è½¬æ¢ä¸ºæ•°å€¼')

        out_of_range = df[
            (df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'] < -1000000) |
            (df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'] > 100000)
        ]
        if len(out_of_range) > 0:
            errors.append(f'ç­¾å•/æ‰¹æ”¹ä¿è´¹: {len(out_of_range)}è¡Œè¶…å‡ºåˆç†èŒƒå›´')

    # ç­¾å•æ•°é‡æ ¡éªŒ
    if 'ç­¾å•æ•°é‡' in df.columns:
        df['ç­¾å•æ•°é‡'] = pd.to_numeric(df['ç­¾å•æ•°é‡'], errors='coerce')
        invalid = (df['ç­¾å•æ•°é‡'] < 1) | (df['ç­¾å•æ•°é‡'] > 10000)
        if invalid.sum() > 0:
            errors.append(f'ç­¾å•æ•°é‡: {invalid.sum()}è¡Œä¸åœ¨[1, 10000]èŒƒå›´å†…')

    return {
        'valid': len(errors) == 0,
        'errors': errors,
        'message': '; '.join(errors) if errors else 'æ•°å€¼ç±»å‹æ ¡éªŒé€šè¿‡'
    }
```

### 2.3 èŒƒå›´æ£€æŸ¥ (Range Validation)

**ä¼˜å…ˆçº§**: P1 (è­¦å‘Šæ€§)

```python
FIELD_RANGES = {
    'ç­¾å•/æ‰¹æ”¹ä¿è´¹': {'min': -1000000, 'max': 100000, 'allow_negative': True},
    'ç­¾å•æ•°é‡': {'min': 1, 'max': 10000, 'allow_negative': False},
    'æ‰‹ç»­è´¹å«ç¨': {'min': 0, 'max': 50000, 'allow_negative': False},
    'æ‰‹ç»­è´¹æ¯”ä¾‹': {'min': 0, 'max': 1, 'allow_negative': False},
}

def validate_ranges(df):
    """
    èŒƒå›´æ£€æŸ¥

    Returns:
        dict: {
            'valid': bool,
            'warnings': list,
            'out_of_range_count': dict
        }
    """
    warnings = []
    out_of_range_count = {}

    for field, config in FIELD_RANGES.items():
        if field not in df.columns:
            continue

        # æ£€æŸ¥èŒƒå›´
        valid_data = df[field].dropna()
        min_val = config['min']
        max_val = config['max']

        if not config['allow_negative']:
            out_of_range = valid_data[(valid_data < min_val) | (valid_data > max_val)]
        else:
            out_of_range = valid_data[(valid_data < min_val) | (valid_data > max_val)]

        count = len(out_of_range)
        if count > 0:
            out_of_range_count[field] = count
            warnings.append(f'{field}: {count}è¡Œè¶…å‡ºèŒƒå›´[{min_val}, {max_val}]')

    return {
        'valid': len(warnings) == 0,
        'warnings': warnings,
        'out_of_range_count': out_of_range_count,
        'message': '; '.join(warnings) if warnings else 'èŒƒå›´æ£€æŸ¥é€šè¿‡'
    }
```

### 2.4 ä¸€è‡´æ€§æ ¡éªŒ (Consistency Validation)

**ä¼˜å…ˆçº§**: P1 (è­¦å‘Šæ€§)

**å®ç°ä½ç½®**: [backend/data_processor.py:771-819](../../../backend/data_processor.py#L771-L819)

```python
def validate_policy_consistency(df, staff_mapping):
    """
    ä¿å•â†’ä¸šåŠ¡å‘˜â†’æœºæ„/å›¢é˜Ÿ ä¸€è‡´æ€§æ ¡éªŒ

    è§„åˆ™:
    - åŒä¸€ä¿å•å·çš„ä¸šåŠ¡å‘˜åº”è¯¥ä¸€è‡´
    - ä¸šåŠ¡å‘˜çš„ä¸‰çº§æœºæ„/å›¢é˜Ÿåº”ä¸æ˜ å°„è¡¨ä¸€è‡´

    Returns:
        dict: {
            'valid': bool,
            'mismatch_policies': list,
            'mismatch_count': int,
            'conflicts': list
        }
    """
    mismatch_policies = []

    # 1. æ£€æŸ¥ä¿å•å·â†’ä¸šåŠ¡å‘˜ä¸€è‡´æ€§
    if 'ä¿å•å·' in df.columns and 'ä¸šåŠ¡å‘˜' in df.columns:
        policy_staff = df.groupby('ä¿å•å·')['ä¸šåŠ¡å‘˜'].nunique()
        inconsistent = policy_staff[policy_staff > 1]
        if len(inconsistent) > 0:
            mismatch_policies.extend(inconsistent.index.tolist())

    # 2. æ£€æŸ¥ä¸šåŠ¡å‘˜â†’æœºæ„/å›¢é˜Ÿä¸€è‡´æ€§
    name_to_info, conflicts = _build_name_to_info(staff_mapping)

    if 'ä¸‰çº§æœºæ„' in df.columns and 'ä¸šåŠ¡å‘˜' in df.columns:
        for idx, row in df.iterrows():
            staff_name = row['ä¸šåŠ¡å‘˜']
            data_institution = row['ä¸‰çº§æœºæ„']

            if staff_name in name_to_info:
                mapped_institution = name_to_info[staff_name].get('ä¸‰çº§æœºæ„')
                if mapped_institution and data_institution != mapped_institution:
                    mismatch_policies.append(row.get('ä¿å•å·', f'Row-{idx}'))

    return {
        'valid': len(mismatch_policies) == 0 and len(conflicts) == 0,
        'mismatch_policies': mismatch_policies[:10],  # åªè¿”å›å‰10æ¡
        'mismatch_count': len(mismatch_policies),
        'conflicts': conflicts,
        'message': f'å‘ç°{len(mismatch_policies)}æ¡ä¸ä¸€è‡´è®°å½•,{len(conflicts)}ä¸ªå§“åå†²çª' if mismatch_policies or conflicts else 'ä¸€è‡´æ€§æ ¡éªŒé€šè¿‡'
    }
```

---

## ğŸ§¹ ä¸‰ã€æ•°æ®æ¸…æ´—è§„èŒƒ (Data Cleaning Standards)

### 3.1 ç¼ºå¤±å€¼å¤„ç† (Missing Value Handling)

**å®ç°ä½ç½®**: [backend/data_processor.py:132-156](../../../backend/data_processor.py#L132-L156)

| å­—æ®µç±»å‹ | ç¼ºå¤±å€¼ç­–ç•¥ | å¡«å……å€¼ | ä¸šåŠ¡è§„åˆ™ |
|----------|------------|--------|----------|
| ä¸‰çº§æœºæ„ | ä»æ˜ å°„è¡¨æŸ¥æ‰¾ | æ˜ å°„è¡¨å€¼ | ä¼˜å…ˆä½¿ç”¨ä¸šåŠ¡å‘˜â†’æœºæ„æ˜ å°„ |
| å›¢é˜Ÿç®€ç§° | ä¿ç•™ç¼ºå¤± | `''` (ç©ºå­—ç¬¦ä¸²) | å…è®¸ä¸ºç©º |
| ç­¾å•/æ‰¹æ”¹ä¿è´¹ | æ ‡è®°ä¸ºæ— æ•ˆ | ä¸å¡«å……,åˆ é™¤è¡Œ | é˜»æ–­æ€§é”™è¯¯ |
| æ‰‹ç»­è´¹å«ç¨ | å¡«å……ä¸º0 | `0` | å…è®¸é›¶æ‰‹ç»­è´¹ |
| æ˜¯å¦ç»­ä¿ | ä¿ç•™ç¼ºå¤± | `''` | å‰ç«¯æ˜¾ç¤º"æœªçŸ¥" |
| æ•°å€¼å­—æ®µ | è½¬æ¢å¤±è´¥è®¾ä¸ºNaN | `NaN` | åç»­è¿‡æ»¤æˆ–å¡«å……0 |
| å­—ç¬¦ä¸²å­—æ®µ | å¡«å……ç©ºå­—ç¬¦ä¸² | `''` | é¿å…Noneå¯¼è‡´çš„å¼‚å¸¸ |

#### å®ç°ç¤ºä¾‹

```python
def handle_missing_values(df, staff_mapping):
    """
    ç¼ºå¤±å€¼å¤„ç†

    Args:
        df: åŸå§‹DataFrame
        staff_mapping: ä¸šåŠ¡å‘˜æ˜ å°„å­—å…¸

    Returns:
        DataFrame: å¤„ç†åçš„æ•°æ®
    """
    # 1. ä¸‰çº§æœºæ„ç¼ºå¤± â†’ ä»æ˜ å°„è¡¨æŸ¥æ‰¾
    if 'ä¸‰çº§æœºæ„' in df.columns and 'ä¸šåŠ¡å‘˜' in df.columns:
        name_to_info, _ = _build_name_to_info(staff_mapping)

        missing_mask = df['ä¸‰çº§æœºæ„'].isnull() | (df['ä¸‰çº§æœºæ„'] == '')
        for idx in df[missing_mask].index:
            staff_name = df.at[idx, 'ä¸šåŠ¡å‘˜']
            if staff_name in name_to_info:
                df.at[idx, 'ä¸‰çº§æœºæ„'] = name_to_info[staff_name].get('ä¸‰çº§æœºæ„', '')

    # 2. æ‰‹ç»­è´¹ç¼ºå¤± â†’ å¡«å……0
    if 'æ‰‹ç»­è´¹å«ç¨' in df.columns:
        df['æ‰‹ç»­è´¹å«ç¨'] = df['æ‰‹ç»­è´¹å«ç¨'].fillna(0)

    # 3. ç­¾å•ä¿è´¹ç¼ºå¤± â†’ åˆ é™¤è¡Œ
    if 'ç­¾å•/æ‰¹æ”¹ä¿è´¹' in df.columns:
        before_count = len(df)
        df = df[df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'].notnull()]
        after_count = len(df)
        if before_count > after_count:
            print(f'âš ï¸  åˆ é™¤{before_count - after_count}è¡Œä¿è´¹ç¼ºå¤±æ•°æ®')

    # 4. å­—ç¬¦ä¸²å­—æ®µ â†’ å¡«å……ç©ºå­—ç¬¦ä¸²
    string_columns = df.select_dtypes(include=['object']).columns
    df[string_columns] = df[string_columns].fillna('')

    return df
```

### 3.2 å¼‚å¸¸å€¼å¤„ç† (Outlier Handling)

**ä¼˜å…ˆçº§**: P1 (è­¦å‘Šæ€§)

| å¼‚å¸¸ç±»å‹ | æ£€æµ‹è§„åˆ™ | å¤„ç†ç­–ç•¥ | ä¸šåŠ¡å«ä¹‰ |
|----------|----------|----------|----------|
| è´Ÿä¿è´¹ | `ç­¾å•/æ‰¹æ”¹ä¿è´¹ < 0` | âœ… **ä¿ç•™** | é€€ä¿/æ‰¹æ”¹è°ƒæ•´,åˆæ³• |
| é›¶æ‰‹ç»­è´¹ | `æ‰‹ç»­è´¹å«ç¨ == 0` | âœ… **ä¿ç•™** | æ­£å¸¸ä¸šåŠ¡åœºæ™¯ |
| è´Ÿä¿é¢ | `ç­¾å•/æ‰¹æ”¹ä¿é¢ < 0` | âš ï¸ **æ ‡è®°** | æ•°æ®å¼‚å¸¸,éœ€äººå·¥å¤æ ¸ |
| è¶…å¤§ä¿è´¹ | `ç­¾å•/æ‰¹æ”¹ä¿è´¹ > 100000` | âš ï¸ **æ ‡è®°** | å¯èƒ½æ•°æ®é”™è¯¯ |
| å¼‚å¸¸æ‰‹ç»­è´¹æ¯”ä¾‹ | `æ‰‹ç»­è´¹æ¯”ä¾‹ < 0.03 or > 0.08` | âš ï¸ **æ ‡è®°** | å¼‚å¸¸ä¸šåŠ¡ |

**å…³é”®åŸåˆ™**:
> **NEVER filter out negative premium values.** Negative premiums are legitimate business data representing policy cancellations, adjustments, or refunds. They MUST be included in all calculations.

#### å®ç°ç¤ºä¾‹

```python
def detect_outliers(df):
    """
    å¼‚å¸¸å€¼æ£€æµ‹(ä»…æ ‡è®°,ä¸åˆ é™¤)

    Returns:
        dict: {
            'outlier_counts': dict,
            'outlier_details': list,
            'warnings': list
        }
    """
    outlier_counts = {}
    outlier_details = []
    warnings = []

    # 1. è´Ÿä¿é¢æ£€æµ‹
    if 'ç­¾å•/æ‰¹æ”¹ä¿é¢' in df.columns:
        negative_amount = df[df['ç­¾å•/æ‰¹æ”¹ä¿é¢'] < 0]
        count = len(negative_amount)
        if count > 0:
            outlier_counts['è´Ÿä¿é¢'] = count
            warnings.append(f'âš ï¸  å‘ç°{count}æ¡è´Ÿä¿é¢è®°å½•,è¯·äººå·¥å¤æ ¸')
            outlier_details.extend(negative_amount['ä¿å•å·'].tolist()[:5])

    # 2. è¶…å¤§ä¿è´¹æ£€æµ‹
    if 'ç­¾å•/æ‰¹æ”¹ä¿è´¹' in df.columns:
        large_premium = df[df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'] > 100000]
        count = len(large_premium)
        if count > 0:
            outlier_counts['è¶…å¤§ä¿è´¹'] = count
            warnings.append(f'âš ï¸  å‘ç°{count}æ¡ä¿è´¹>10ä¸‡è®°å½•')

    # 3. å¼‚å¸¸æ‰‹ç»­è´¹æ¯”ä¾‹
    if 'æ‰‹ç»­è´¹æ¯”ä¾‹' in df.columns:
        abnormal_ratio = df[
            (df['æ‰‹ç»­è´¹æ¯”ä¾‹'] < 0.03) | (df['æ‰‹ç»­è´¹æ¯”ä¾‹'] > 0.08)
        ]
        count = len(abnormal_ratio)
        if count > 0:
            outlier_counts['å¼‚å¸¸æ‰‹ç»­è´¹æ¯”ä¾‹'] = count
            warnings.append(f'âš ï¸  å‘ç°{count}æ¡æ‰‹ç»­è´¹æ¯”ä¾‹å¼‚å¸¸(< 3% or > 8%)')

    return {
        'outlier_counts': outlier_counts,
        'outlier_details': outlier_details[:10],
        'warnings': warnings,
        'message': '\n'.join(warnings) if warnings else 'æœªå‘ç°å¼‚å¸¸å€¼'
    }
```

### 3.3 é‡å¤æ•°æ®å¤„ç† (Duplicate Handling)

**å®ç°ä½ç½®**: [backend/data_processor.py:158-192](../../../backend/data_processor.py#L158-L192)

**å»é‡è§„åˆ™**:
- **è”åˆä¸»é”®**: `ä¿å•å·` + `æŠ•ä¿ç¡®è®¤æ—¶é—´`
- **ä¿ç•™ç­–ç•¥**: `keep='last'` (ä¿ç•™æœ€æ–°è®°å½•)
- **ç†ç”±**: åŒä¸€ä¿å•åœ¨ä¸åŒæ—¶é—´å¯èƒ½æœ‰æ‰¹æ”¹æ›´æ–°

```python
def remove_duplicates(df):
    """
    å»é‡å¤„ç†

    è§„åˆ™:
    - è”åˆä¸»é”®: ä¿å•å· + æŠ•ä¿ç¡®è®¤æ—¶é—´
    - ä¿ç•™æœ€æ–°è®°å½•(keep='last')

    Returns:
        DataFrame: å»é‡åçš„æ•°æ®
    """
    if 'ä¿å•å·' not in df.columns or 'æŠ•ä¿ç¡®è®¤æ—¶é—´' not in df.columns:
        return df

    before_count = len(df)

    # ç¡®ä¿æ—¥æœŸç±»å‹
    try:
        df['æŠ•ä¿ç¡®è®¤æ—¶é—´'] = pd.to_datetime(df['æŠ•ä¿ç¡®è®¤æ—¶é—´'], errors='coerce')
    except Exception:
        pass

    # ä½¿ç”¨ duplicated ç”Ÿæˆæ©ç ,ä¿ç•™æœ€åä¸€æ¡
    dup_mask = df.duplicated(subset=['ä¿å•å·', 'æŠ•ä¿ç¡®è®¤æ—¶é—´'], keep='last')
    df = df[~dup_mask]

    after_count = len(df)

    if before_count > after_count:
        print(f'â„¹ï¸  å»é‡: {before_count} â†’ {after_count} (åˆ é™¤{before_count - after_count}æ¡)')

    return df
```

### 3.4 æ•°æ®æ ‡å‡†åŒ– (Data Normalization)

**å®ç°ä½ç½®**: [backend/data_processor.py:132-156](../../../backend/data_processor.py#L132-L156)

| å­—æ®µç±»å‹ | æ ‡å‡†åŒ–è§„åˆ™ | å®ç°æ–¹æ³• |
|----------|------------|----------|
| æ—¥æœŸå­—æ®µ | ç»Ÿä¸€ä¸º `datetime64[ns]` | `pd.to_datetime(errors='coerce')` |
| æ•°å€¼å­—æ®µ | ç»Ÿä¸€ä¸º `float64` | `pd.to_numeric(errors='coerce')` |
| å­—ç¬¦ä¸²å­—æ®µ | å»é™¤é¦–å°¾ç©ºæ ¼ | `str.strip()` |
| æ˜¯å¦ç±»å­—æ®µ | ç»Ÿä¸€ä¸º"æ˜¯"/"å¦" | æ˜ å°„ `{'Y':'æ˜¯', 'N':'å¦'}` |
| æœºæ„åç§° | å»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦ | `str.replace()` |

```python
def normalize_data(df):
    """
    æ•°æ®æ ‡å‡†åŒ–

    Returns:
        DataFrame: æ ‡å‡†åŒ–åçš„æ•°æ®
    """
    # 1. æ—¥æœŸå­—æ®µæ ‡å‡†åŒ–
    date_columns = ['åˆ·æ–°æ—¶é—´', 'æŠ•ä¿ç¡®è®¤æ—¶é—´', 'ä¿é™©èµ·æœŸ']
    for col in date_columns:
        if col in df.columns:
            df[col] = pd.to_datetime(df[col], errors='coerce')

    # 2. æ•°å€¼å­—æ®µæ ‡å‡†åŒ–
    numeric_columns = ['ç­¾å•/æ‰¹æ”¹ä¿è´¹', 'ç­¾å•æ•°é‡', 'æ‰‹ç»­è´¹', 'æ‰‹ç»­è´¹å«ç¨', 'å¢å€¼ç¨']
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    # 3. å­—ç¬¦ä¸²å­—æ®µå»é™¤ç©ºæ ¼
    string_columns = df.select_dtypes(include=['object']).columns
    for col in string_columns:
        df[col] = df[col].astype(str).str.strip()

    # 4. æ˜¯å¦ç±»å­—æ®µæ ‡å‡†åŒ–
    yes_no_columns = ['æ˜¯å¦ç»­ä¿', 'æ˜¯å¦æ–°èƒ½æº', 'æ˜¯å¦è¿‡æˆ·è½¦', 'æ˜¯å¦å¼‚åœ°è½¦', 'æ˜¯å¦ç½‘çº¦è½¦']
    for col in yes_no_columns:
        if col in df.columns:
            df[col] = df[col].map({'Y': 'æ˜¯', 'N': 'å¦', 'y': 'æ˜¯', 'n': 'å¦'}).fillna(df[col])

    return df
```

---

## ğŸ—‚ï¸ å››ã€æ˜ å°„ç®¡ç† (Mapping Management)

### 4.1 ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿæ˜ å°„

**æ ¸å¿ƒæ–‡ä»¶**: `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±.json` (229 records as of 2025-11-04)

**æ•°æ®ç»“æ„**:
```json
{
  "200049147å‘è½©é¢‰": {
    "ä¸‰çº§æœºæ„": "è¾¾å·",
    "å››çº§æœºæ„": "è¾¾å·",
    "å›¢é˜Ÿç®€ç§°": null
  },
  "210011936èµµèè": {
    "ä¸‰çº§æœºæ„": "è¾¾å·",
    "å››çº§æœºæ„": "è¾¾å·",
    "å›¢é˜Ÿç®€ç§°": "è¾¾å·ä¸šåŠ¡ä¸‰éƒ¨"
  }
}
```

**å…³é”®è§„åˆ™**:
- **æ˜ å°„ä¼˜å…ˆçº§**: æ˜ å°„è¡¨ > æ•°æ®æ–‡ä»¶ä¸­çš„æœºæ„å­—æ®µ
- **ä¸ºä»€ä¹ˆ**: æ•°æ®æ–‡ä»¶ä¸­çš„ä¸‰çº§æœºæ„å¯èƒ½ä¸å‡†ç¡®,æ˜ å°„è¡¨æ˜¯æƒå¨æ•°æ®æº
- **å†²çªè§£å†³**: åŒåä¸åŒæœºæ„æ—¶,ä¿ç•™æœ€åä¸€æ¡å¹¶æ ‡è®°å†²çª

### 4.2 æ˜ å°„æ›´æ–°æµç¨‹

**æ­¥éª¤**:

1. **æ¥æ”¶æ–°æ˜ å°„æ–‡ä»¶** (Excelæ ¼å¼)
   - æ–‡ä»¶å: `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå¯¹ç…§è¡¨YYYYMMDD.xlsx`
   - å¿…éœ€åˆ—: åºå·, ä¸‰çº§æœºæ„, å››çº§æœºæ„, å›¢é˜Ÿç®€ç§°, ä¸šåŠ¡å‘˜

2. **è½¬æ¢ä¸ºJSONæ ¼å¼**
   ```python
   def convert_staff_mapping_excel_to_json(excel_path, json_path):
       """
       è½¬æ¢ä¸šåŠ¡å‘˜æ˜ å°„è¡¨ Excel â†’ JSON

       Args:
           excel_path: Excelæ–‡ä»¶è·¯å¾„
           json_path: è¾“å‡ºJSONè·¯å¾„
       """
       df = pd.read_excel(excel_path)

       # æå–å…³é”®åˆ—
       required_columns = ['ä¸šåŠ¡å‘˜', 'ä¸‰çº§æœºæ„', 'å››çº§æœºæ„', 'å›¢é˜Ÿç®€ç§°']
       if not all(col in df.columns for col in required_columns):
           raise ValueError(f'Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {required_columns}')

       # æ„å»ºæ˜ å°„å­—å…¸
       mapping = {}
       for _, row in df.iterrows():
           staff_key = str(row['ä¸šåŠ¡å‘˜'])  # æ ¼å¼: å·¥å·+å§“å
           mapping[staff_key] = {
               'ä¸‰çº§æœºæ„': str(row['ä¸‰çº§æœºæ„']),
               'å››çº§æœºæ„': str(row['å››çº§æœºæ„']),
               'å›¢é˜Ÿç®€ç§°': str(row['å›¢é˜Ÿç®€ç§°']) if pd.notna(row['å›¢é˜Ÿç®€ç§°']) else None
           }

       # ä¿å­˜JSON
       with open(json_path, 'w', encoding='utf-8') as f:
           json.dump(mapping, f, ensure_ascii=False, indent=2)

       print(f'âœ… æ˜ å°„è¡¨è½¬æ¢å®Œæˆ: {len(mapping)} æ¡è®°å½•')
       return mapping
   ```

3. **éªŒè¯æ˜ å°„å®Œæ•´æ€§**
   ```python
   def validate_staff_mapping(df, staff_mapping):
       """
       éªŒè¯æ•°æ®ä¸­çš„ä¸šåŠ¡å‘˜æ˜¯å¦éƒ½å­˜åœ¨äºæ˜ å°„è¡¨

       Returns:
           dict: {
               'unmatched_staff': list,
               'unmatched_count': int,
               'coverage_rate': float
           }
       """
       if 'ä¸šåŠ¡å‘˜' not in df.columns:
           return {'unmatched_staff': [], 'unmatched_count': 0, 'coverage_rate': 1.0}

       # æå–å§“å
       import re
       name_to_info = {}
       for staff_key in staff_mapping.keys():
           match = re.search(r'[\u4e00-\u9fa5]+', staff_key)
           if match:
               name = match.group()
               name_to_info[name] = staff_key

       # æ£€æŸ¥æœªåŒ¹é…
       data_staff = df['ä¸šåŠ¡å‘˜'].unique()
       unmatched = [s for s in data_staff if s not in name_to_info]

       coverage_rate = 1.0 - (len(unmatched) / len(data_staff)) if len(data_staff) > 0 else 1.0

       return {
           'unmatched_staff': unmatched[:10],
           'unmatched_count': len(unmatched),
           'coverage_rate': coverage_rate,
           'message': f'æ˜ å°„è¦†ç›–ç‡: {coverage_rate*100:.1f}% ({len(data_staff)-len(unmatched)}/{len(data_staff)})'
       }
   ```

4. **æ›´æ–°ç³»ç»Ÿæ˜ å°„æ–‡ä»¶**
   - å¤‡ä»½æ—§ç‰ˆæœ¬: `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±_backup_YYYYMMDD.json`
   - æ›¿æ¢ä¸ºæ–°ç‰ˆæœ¬
   - è§¦å‘æ•°æ®åˆ·æ–°

5. **éªŒè¯æ›´æ–°æ•ˆæœ**
   - é‡æ–°åŠ è½½æ•°æ®
   - æ£€æŸ¥æœªåŒ¹é…ä¸šåŠ¡å‘˜æ•°é‡
   - ç”Ÿæˆæ›´æ–°æŠ¥å‘Š

### 4.3 å†²çªè§£å†³ç­–ç•¥

**å†²çªåœºæ™¯**: åŒä¸€å§“åå‡ºç°åœ¨å¤šæ¡è®°å½•ä¸­,ä½†æœºæ„/å›¢é˜Ÿä¿¡æ¯ä¸åŒ

**ç¤ºä¾‹**:
```json
{
  "200012345å¼ ä¸‰": {"ä¸‰çº§æœºæ„": "è¾¾å·", "å›¢é˜Ÿç®€ç§°": "ä¸šåŠ¡ä¸€éƒ¨"},
  "210067890å¼ ä¸‰": {"ä¸‰çº§æœºæ„": "å¾·é˜³", "å›¢é˜Ÿç®€ç§°": "ä¸šåŠ¡äºŒéƒ¨"}
}
```

**è§£å†³ç­–ç•¥**:

1. **è‡ªåŠ¨å¤„ç†** (å½“å‰ç­–ç•¥)
   - ä¿ç•™æœ€åä¸€æ¡è®°å½•
   - è®°å½•å†²çªåˆ—è¡¨,è¿”å›å‰ç«¯æç¤º
   - å®ç°ä½ç½®: [backend/data_processor.py:23-58](../../../backend/data_processor.py#L23-L58)

2. **äººå·¥ä»‹å…¥** (æ¨è,æœªæ¥å®ç°)
   - å‰ç«¯æ˜¾ç¤ºå†²çªåˆ—è¡¨
   - è¦æ±‚ç®¡ç†å‘˜é€‰æ‹©æ­£ç¡®è®°å½•
   - æˆ–å»ºè®®åœ¨æ˜ å°„è¡¨ä¸­åŒºåˆ†(æ·»åŠ å·¥å·)

3. **å·¥å·åŒºåˆ†** (æœ€ä½³å®è·µ)
   - åœ¨æ˜¾ç¤ºæ—¶ä½¿ç”¨"å·¥å·+å§“å"
   - ç³»ç»Ÿå†…éƒ¨ç»´æŠ¤å·¥å·â†’å§“åæ˜ å°„
   - é¿å…åŒåå†²çª

### 4.4 å†å²ç‰ˆæœ¬ç®¡ç†

**ç‰ˆæœ¬å‘½åè§„åˆ™**:
- æ ¼å¼: `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±_vYYYYMMDD.json`
- ç¤ºä¾‹: `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±_v20251104.json`

**ä¿ç•™ç­–ç•¥**:
- ä¿ç•™æœ€è¿‘12ä¸ªæœˆçš„ç‰ˆæœ¬
- æ¯æœˆ1å·è‡ªåŠ¨å½’æ¡£
- è¶…è¿‡12ä¸ªæœˆçš„ç‰ˆæœ¬ç§»è‡³ `data/archive/`

**ç‰ˆæœ¬å¯¹æ¯”**:
```python
def compare_mapping_versions(old_json, new_json):
    """
    å¯¹æ¯”ä¸¤ä¸ªç‰ˆæœ¬çš„æ˜ å°„æ–‡ä»¶

    Returns:
        dict: {
            'added': list,      # æ–°å¢ä¸šåŠ¡å‘˜
            'removed': list,    # åˆ é™¤ä¸šåŠ¡å‘˜
            'changed': list,    # æœºæ„/å›¢é˜Ÿå˜æ›´
            'unchanged': int
        }
    """
    with open(old_json, 'r', encoding='utf-8') as f:
        old_mapping = json.load(f)

    with open(new_json, 'r', encoding='utf-8') as f:
        new_mapping = json.load(f)

    old_keys = set(old_mapping.keys())
    new_keys = set(new_mapping.keys())

    added = list(new_keys - old_keys)
    removed = list(old_keys - new_keys)

    changed = []
    for key in old_keys & new_keys:
        if old_mapping[key] != new_mapping[key]:
            changed.append({
                'staff': key,
                'old': old_mapping[key],
                'new': new_mapping[key]
            })

    return {
        'added': added,
        'removed': removed,
        'changed': changed,
        'unchanged': len(old_keys & new_keys) - len(changed),
        'summary': f'æ–°å¢{len(added)},åˆ é™¤{len(removed)},å˜æ›´{len(changed)},ä¸å˜{len(old_keys & new_keys) - len(changed)}'
    }
```

---

## ğŸ“Š äº”ã€æ•°æ®å®¡è®¡ (Data Audit)

### 5.1 å˜æ›´æ—¥å¿— (Change Log)

**æ—¥å¿—ä½ç½®**: `logs/data_audit.log`

**è®°å½•å†…å®¹**:
- æ•°æ®åˆ·æ–°æ—¶é—´
- æ–‡ä»¶æ¥æº(Excelæ–‡ä»¶å)
- å¤„ç†å‰/åè®°å½•æ•°
- å»é‡/æ¸…æ´—ç»Ÿè®¡
- å¼‚å¸¸å€¼æ ‡è®°
- æ˜ å°„åŒ¹é…ç‡

**æ—¥å¿—æ ¼å¼**:
```
[2025-11-08 14:30:25] [INFO] æ•°æ®åˆ·æ–°å¼€å§‹
[2025-11-08 14:30:26] [INFO] åŠ è½½Excel: è½¦é™©æ¸…å•_202511.xlsx
[2025-11-08 14:30:28] [INFO] æ•°æ®æ¸…æ´—: 5234 â†’ 5180 (åˆ é™¤54è¡Œ)
[2025-11-08 14:30:29] [INFO] å»é‡å¤„ç†: 5180 â†’ 5123 (åˆ é™¤57è¡Œ)
[2025-11-08 14:30:30] [WARN] å‘ç°23æ¡è´Ÿä¿é¢è®°å½•,å·²æ ‡è®°
[2025-11-08 14:30:31] [INFO] æ˜ å°„åŒ¹é…ç‡: 98.5% (5045/5123)
[2025-11-08 14:30:32] [INFO] æ•°æ®åˆ·æ–°å®Œæˆ,æœ€æ–°æ—¥æœŸ: 2025-11-08
```

### 5.2 è´¨é‡æŠ¥å‘Š (Quality Report)

**ç”Ÿæˆé¢‘ç‡**: æ¯æ¬¡æ•°æ®åˆ·æ–°åè‡ªåŠ¨ç”Ÿæˆ

**æŠ¥å‘Šå†…å®¹**:

```python
def generate_quality_report(df, staff_mapping):
    """
    ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

    Returns:
        dict: è´¨é‡æŠ¥å‘Š
    """
    report = {
        'timestamp': datetime.now().isoformat(),
        'data_summary': {
            'total_records': len(df),
            'date_range': f"{df['æŠ•ä¿ç¡®è®¤æ—¶é—´'].min()} ~ {df['æŠ•ä¿ç¡®è®¤æ—¶é—´'].max()}",
            'latest_date': df['æŠ•ä¿ç¡®è®¤æ—¶é—´'].max().strftime('%Y-%m-%d')
        },
        'field_completeness': {},
        'validation_results': {},
        'outlier_detection': {},
        'mapping_coverage': {},
        'quality_score': 0.0
    }

    # 1. å­—æ®µå®Œæ•´æ€§
    for col in df.columns:
        null_count = df[col].isnull().sum()
        completeness = 1.0 - (null_count / len(df))
        report['field_completeness'][col] = {
            'completeness': completeness,
            'null_count': null_count,
            'status': 'âœ…' if completeness >= 0.95 else 'âš ï¸' if completeness >= 0.8 else 'âŒ'
        }

    # 2. å¿…å¡«å­—æ®µæ ¡éªŒ
    report['validation_results']['required_fields'] = validate_required_fields(df)

    # 3. æ ¼å¼éªŒè¯
    report['validation_results']['date_format'] = validate_date_format(df)
    report['validation_results']['numeric_types'] = validate_numeric_types(df)

    # 4. å¼‚å¸¸å€¼æ£€æµ‹
    report['outlier_detection'] = detect_outliers(df)

    # 5. æ˜ å°„è¦†ç›–ç‡
    report['mapping_coverage'] = validate_staff_mapping(df, staff_mapping)

    # 6. ç»¼åˆè´¨é‡è¯„åˆ† (0-100)
    score = 0
    score += 30 if report['validation_results']['required_fields']['valid'] else 0
    score += 20 if report['validation_results']['date_format']['valid'] else 0
    score += 20 if report['validation_results']['numeric_types']['valid'] else 0
    score += 15 if report['mapping_coverage']['coverage_rate'] >= 0.95 else 10 if report['mapping_coverage']['coverage_rate'] >= 0.9 else 0
    score += 15 if len(report['outlier_detection']['outlier_counts']) == 0 else 5

    report['quality_score'] = score
    report['quality_level'] = 'ä¼˜ç§€' if score >= 90 else 'è‰¯å¥½' if score >= 75 else 'åŠæ ¼' if score >= 60 else 'ä¸åŠæ ¼'

    return report
```

**æŠ¥å‘Šç¤ºä¾‹**:

```json
{
  "timestamp": "2025-11-08T14:30:32",
  "data_summary": {
    "total_records": 5123,
    "date_range": "2025-10-01 ~ 2025-11-08",
    "latest_date": "2025-11-08"
  },
  "field_completeness": {
    "æŠ•ä¿ç¡®è®¤æ—¶é—´": {"completeness": 1.0, "null_count": 0, "status": "âœ…"},
    "ä¸‰çº§æœºæ„": {"completeness": 0.985, "null_count": 77, "status": "âœ…"},
    "å›¢é˜Ÿç®€ç§°": {"completeness": 0.72, "null_count": 1434, "status": "âš ï¸"}
  },
  "validation_results": {
    "required_fields": {"valid": true, "message": "æ‰€æœ‰å¿…å¡«å­—æ®µæ ¡éªŒé€šè¿‡"},
    "date_format": {"valid": true, "message": "æ—¥æœŸæ ¼å¼æ ¡éªŒé€šè¿‡"},
    "numeric_types": {"valid": true, "message": "æ•°å€¼ç±»å‹æ ¡éªŒé€šè¿‡"}
  },
  "outlier_detection": {
    "outlier_counts": {"è´Ÿä¿é¢": 23, "å¼‚å¸¸æ‰‹ç»­è´¹æ¯”ä¾‹": 12},
    "warnings": ["âš ï¸  å‘ç°23æ¡è´Ÿä¿é¢è®°å½•,è¯·äººå·¥å¤æ ¸"]
  },
  "mapping_coverage": {
    "unmatched_count": 8,
    "coverage_rate": 0.985,
    "message": "æ˜ å°„è¦†ç›–ç‡: 98.5% (5045/5123)"
  },
  "quality_score": 85,
  "quality_level": "è‰¯å¥½"
}
```

### 5.3 å¼‚å¸¸å‘Šè­¦ (Anomaly Alerts)

**å‘Šè­¦è§¦å‘æ¡ä»¶**:

| å‘Šè­¦çº§åˆ« | è§¦å‘æ¡ä»¶ | é€šçŸ¥æ–¹å¼ |
|----------|----------|----------|
| ğŸ”´ ä¸¥é‡ | è´¨é‡è¯„åˆ† < 60 | å‰ç«¯çº¢è‰²æç¤º |
| ğŸŸ¡ è­¦å‘Š | è´¨é‡è¯„åˆ† 60-74 | å‰ç«¯é»„è‰²æç¤º |
| âš ï¸  æé†’ | å¼‚å¸¸å€¼æ•°é‡ > 50 | å‰ç«¯ç°è‰²æç¤º |
| â„¹ï¸  ä¿¡æ¯ | æ˜ å°„è¦†ç›–ç‡ < 95% | æ—¥å¿—è®°å½• |

**å‘Šè­¦æ¶ˆæ¯æ¨¡æ¿**:

```python
ALERT_TEMPLATES = {
    'low_quality_score': 'âš ï¸ æ•°æ®è´¨é‡è¯„åˆ†ä½({score}åˆ†),è¯·æ£€æŸ¥æ•°æ®æº',
    'high_outlier_count': 'âš ï¸ å‘ç°{count}æ¡å¼‚å¸¸æ•°æ®,å»ºè®®äººå·¥å¤æ ¸',
    'low_mapping_coverage': 'â„¹ï¸  æ˜ å°„è¦†ç›–ç‡è¾ƒä½({rate}%),å»ºè®®æ›´æ–°æ˜ å°„è¡¨',
    'missing_required_fields': 'ğŸ”´ ç¼ºå°‘å¿…å¡«å­—æ®µ: {fields}',
    'date_format_errors': 'ğŸ”´ æ—¥æœŸæ ¼å¼é”™è¯¯: {errors}',
}

def generate_alert_messages(quality_report):
    """
    æ ¹æ®è´¨é‡æŠ¥å‘Šç”Ÿæˆå‘Šè­¦æ¶ˆæ¯

    Returns:
        list: å‘Šè­¦æ¶ˆæ¯åˆ—è¡¨
    """
    alerts = []

    # 1. è´¨é‡è¯„åˆ†å‘Šè­¦
    score = quality_report['quality_score']
    if score < 75:
        alerts.append({
            'level': 'warning' if score >= 60 else 'error',
            'message': ALERT_TEMPLATES['low_quality_score'].format(score=score),
            'category': 'quality_score'
        })

    # 2. å¼‚å¸¸å€¼å‘Šè­¦
    outlier_count = sum(quality_report['outlier_detection']['outlier_counts'].values())
    if outlier_count > 50:
        alerts.append({
            'level': 'warning',
            'message': ALERT_TEMPLATES['high_outlier_count'].format(count=outlier_count),
            'category': 'outliers'
        })

    # 3. æ˜ å°„è¦†ç›–ç‡å‘Šè­¦
    coverage_rate = quality_report['mapping_coverage']['coverage_rate']
    if coverage_rate < 0.95:
        alerts.append({
            'level': 'info',
            'message': ALERT_TEMPLATES['low_mapping_coverage'].format(rate=coverage_rate*100),
            'category': 'mapping_coverage'
        })

    # 4. å¿…å¡«å­—æ®µå‘Šè­¦
    if not quality_report['validation_results']['required_fields']['valid']:
        missing_fields = quality_report['validation_results']['required_fields']['missing_fields']
        alerts.append({
            'level': 'error',
            'message': ALERT_TEMPLATES['missing_required_fields'].format(fields=', '.join(missing_fields)),
            'category': 'required_fields'
        })

    return alerts
```

---

## ğŸ“– å…­ã€æœ€ä½³å®è·µ (Best Practices)

### 6.1 æ•°æ®æ²»ç†åŸåˆ™

1. **æ˜ å°„è¡¨æƒå¨åŸåˆ™**
   - âœ… å§‹ç»ˆä½¿ç”¨ `ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±.json` ä½œä¸ºæœºæ„/å›¢é˜Ÿçš„æƒå¨æ¥æº
   - âŒ ä¸è¦ç›´æ¥ä½¿ç”¨æ•°æ®æ–‡ä»¶ä¸­çš„ `ä¸‰çº§æœºæ„` å­—æ®µ

2. **è´Ÿå€¼ä¿ç•™åŸåˆ™**
   - âœ… ä¿ç•™è´Ÿä¿è´¹å€¼(é€€ä¿/è°ƒæ•´çš„åˆæ³•æ•°æ®)
   - âŒ ä¸è¦è¿‡æ»¤æˆ–åˆ é™¤è´Ÿä¿è´¹è®°å½•

3. **å¢é‡æ›´æ–°åŸåˆ™**
   - âœ… ä½¿ç”¨ `ä¿å•å·` + `æŠ•ä¿ç¡®è®¤æ—¶é—´` å»é‡,ä¿ç•™æœ€æ–°
   - âŒ ä¸è¦å…¨é‡æ›¿æ¢,é¿å…å†å²æ•°æ®ä¸¢å¤±

4. **è´¨é‡ä¼˜å…ˆåŸåˆ™**
   - âœ… å¿…å¡«å­—æ®µç¼ºå¤±æ—¶é˜»æ–­æ•°æ®å¯¼å…¥
   - âŒ ä¸è¦å¼ºåˆ¶å¡«å……å¯èƒ½é”™è¯¯çš„é»˜è®¤å€¼

### 6.2 å¸¸è§é™·é˜± (Common Pitfalls)

#### âŒ Pitfall 1: ä½¿ç”¨åŸå§‹æœºæ„å­—æ®µ

```python
# âŒ é”™è¯¯ - ç›´æ¥ä½¿ç”¨æ•°æ®ä¸­çš„ä¸‰çº§æœºæ„
df[df['ä¸‰çº§æœºæ„'] == 'è¾¾å·']

# âœ… æ­£ç¡® - ä½¿ç”¨æ˜ å°„è¡¨æŸ¥æ‰¾
name_to_info, _ = _build_name_to_info(staff_mapping)
staff_list = [name for name, info in name_to_info.items()
              if info['ä¸‰çº§æœºæ„'] == 'è¾¾å·']
df[df['ä¸šåŠ¡å‘˜'].isin(staff_list)]
```

#### âŒ Pitfall 2: è¿‡æ»¤è´Ÿä¿è´¹

```python
# âŒ é”™è¯¯ - åˆ é™¤è´Ÿä¿è´¹
df = df[df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'] > 0]

# âœ… æ­£ç¡® - ä¿ç•™è´Ÿä¿è´¹
total_premium = df['ç­¾å•/æ‰¹æ”¹ä¿è´¹'].sum()  # åŒ…å«è´Ÿå€¼
```

#### âŒ Pitfall 3: å¿½ç•¥æ•°æ®éªŒè¯

```python
# âŒ é”™è¯¯ - ç›´æ¥å¤„ç†,æ— éªŒè¯
df = pd.read_excel('data.xlsx')
df.to_csv('output.csv')

# âœ… æ­£ç¡® - å…ˆéªŒè¯,å†å¤„ç†
df = pd.read_excel('data.xlsx')
validation_result = validate_required_fields(df)
if not validation_result['valid']:
    raise ValueError(validation_result['message'])
df.to_csv('output.csv')
```

#### âŒ Pitfall 4: ç¡¬ç¼–ç æ—¥æœŸèŒƒå›´

```python
# âŒ é”™è¯¯ - ç¡¬ç¼–ç 
df = df[df['æŠ•ä¿ç¡®è®¤æ—¶é—´'] >= '2025-10-01']

# âœ… æ­£ç¡® - ç›¸å¯¹æ—¶é—´
latest_date = df['æŠ•ä¿ç¡®è®¤æ—¶é—´'].max()
start_date = latest_date - timedelta(days=30)
df = df[df['æŠ•ä¿ç¡®è®¤æ—¶é—´'] >= start_date]
```

### 6.3 ä»£ç æ£€æŸ¥æ¸…å• (Code Review Checklist)

åœ¨å®ç°æ•°æ®å¤„ç†é€»è¾‘æ—¶,è¯·ç¡®ä¿:

- [ ] å¿…å¡«å­—æ®µå·²æ ¡éªŒ
- [ ] æ—¥æœŸæ ¼å¼å·²ç»Ÿä¸€ä¸º `datetime64[ns]`
- [ ] æ•°å€¼å­—æ®µå·²è½¬æ¢ä¸º `float64`
- [ ] è´Ÿä¿è´¹å€¼å·²ä¿ç•™(æœªè¿‡æ»¤)
- [ ] ä½¿ç”¨æ˜ å°„è¡¨è·å–æœºæ„/å›¢é˜Ÿ(éæ•°æ®æ–‡ä»¶å­—æ®µ)
- [ ] å»é‡ä½¿ç”¨ `duplicated + æ©ç ` (é¿å…ç±»å‹é—®é¢˜)
- [ ] ç¼ºå¤±å€¼å·²æŒ‰è§„åˆ™å¤„ç†(éé»˜è®¤å…¨å¡«å……0)
- [ ] å¼‚å¸¸å€¼å·²æ£€æµ‹å¹¶æ ‡è®°(éåˆ é™¤)
- [ ] æ•°æ®è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ
- [ ] é”™è¯¯æ—¥å¿—å·²è®°å½•

---

## ğŸ“‚ ç›¸å…³æ–‡ä»¶ç´¢å¼• (Related Files)

### æ•°æ®å¤„ç†æ ¸å¿ƒ

- [backend/data_processor.py](../../../backend/data_processor.py) - æ•°æ®å¤„ç†ä¸»é€»è¾‘
  - L23-58: å§“åâ†’æœºæ„æ˜ å°„æ„å»º
  - L59-101: ä¿å•æ˜ å°„ä¿¡æ¯
  - L132-156: æ•°æ®æ¸…æ´—
  - L158-192: å»é‡ä¸åˆå¹¶

### æ•°æ®å­—å…¸

- [docs/FIELD_MAPPING.md](../../../docs/FIELD_MAPPING.md) - å­—æ®µæ˜ å°„è¡¨
- [ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±.json](../../../ä¸šåŠ¡å‘˜æœºæ„å›¢é˜Ÿå½’å±.json) - æ˜ å°„æ–‡ä»¶

### APIä¸æ•°æ®éªŒè¯

- [backend/api_server.py](../../../backend/api_server.py) - APIæ¥å£
  - L141-161: ä¿å•æ˜ å°„æŸ¥è¯¢
  - L244-314: ä¸šç»©åˆ†å¸ƒ(å«è¿‡æ»¤éªŒè¯)

### ç›¸å…³ Skills

- [analyzing-auto-insurance-data](../analyzing-auto-insurance-data/SKILL.md) - æ•°æ®åˆ†æ
- [backend-data-processor](../backend-data-processor/SKILL.md) - åç«¯æ•°æ®å¤„ç†
- [api-endpoint-design](../api-endpoint-design/SKILL.md) - APIè®¾è®¡

---

## ğŸ¯ é¢„æœŸæ”¶ç›Š (Expected Benefits)

### Token èŠ‚çœä¼°ç®—

- **æ¯æ¬¡å¯¹è¯èŠ‚çœ**: 2000-3000 tokens
- **å¹´ä½¿ç”¨æ¬¡æ•°**: çº¦ 30 æ¬¡(æ•°æ®æ²»ç†ç›¸å…³å¯¹è¯)
- **å¹´æ€»èŠ‚çœ**: 60,000 - 90,000 tokens

### è´¨é‡æå‡

- **æ•°æ®å‡†ç¡®æ€§**: ä» 92% æå‡è‡³ 98%+
- **å¼‚å¸¸æ£€æµ‹ç‡**: ä» 60% æå‡è‡³ 95%+
- **æ˜ å°„è¦†ç›–ç‡**: ä» 90% æå‡è‡³ 98%+
- **æ‰‹å·¥å¹²é¢„å‡å°‘**: é™ä½ 70%

### æ•ˆç‡æå‡

- **æ•°æ®å¯¼å…¥è€—æ—¶**: å‡å°‘ 40% (è‡ªåŠ¨éªŒè¯)
- **å¼‚å¸¸æ’æŸ¥æ—¶é—´**: å‡å°‘ 60% (è´¨é‡æŠ¥å‘Š)
- **æ˜ å°„æ›´æ–°æ—¶é—´**: å‡å°‘ 80% (è‡ªåŠ¨åŒ–æµç¨‹)

---

## âœ… æ€»ç»“ (Summary)

### æ ¸å¿ƒè¦ç‚¹

1. **æ•°æ®å­—å…¸**: å®Œæ•´çš„å­—æ®µå®šä¹‰ã€ç±»å‹ã€æšä¸¾å€¼ã€ä¸šåŠ¡å«ä¹‰
2. **è´¨é‡è§„åˆ™**: å¿…å¡«ã€æ ¼å¼ã€èŒƒå›´ã€ä¸€è‡´æ€§å››å±‚æ ¡éªŒ
3. **æ¸…æ´—è§„èŒƒ**: ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€é‡å¤æ•°æ®ã€æ ‡å‡†åŒ–å¤„ç†
4. **æ˜ å°„ç®¡ç†**: æ›´æ–°æµç¨‹ã€å†²çªè§£å†³ã€ç‰ˆæœ¬æ§åˆ¶
5. **æ•°æ®å®¡è®¡**: å˜æ›´æ—¥å¿—ã€è´¨é‡æŠ¥å‘Šã€å¼‚å¸¸å‘Šè­¦

### é€‚ç”¨åœºæ™¯

âœ… **é€‚ç”¨**:
- å®šä¹‰æ•°æ®è´¨é‡æ ‡å‡†
- å®ç°å­—æ®µéªŒè¯é€»è¾‘
- ç®¡ç†ä¸šåŠ¡å‘˜æ˜ å°„æ›´æ–°
- è§£å†³æ•°æ®ä¸ä¸€è‡´é—®é¢˜
- å»ºç«‹æ•°æ®æ²»ç†æµç¨‹
- ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š

âŒ **ä¸é€‚ç”¨**:
- å…·ä½“æ•°æ®åˆ†æä»»åŠ¡ â†’ `analyzing-auto-insurance-data`
- Vue ç»„ä»¶å¼€å‘ â†’ `vue-component-dev`
- API æ¥å£è®¾è®¡ â†’ `api-endpoint-design`

---

**Skill ç‰ˆæœ¬**: v1.0
**åˆ›å»ºæ—¥æœŸ**: 2025-11-09
**æœ€åæ›´æ–°**: 2025-11-09
**ç»´æŠ¤è€…**: Claude Code AI Assistant
**ä¸‹æ¬¡å®¡æŸ¥**: 2025-12-09
