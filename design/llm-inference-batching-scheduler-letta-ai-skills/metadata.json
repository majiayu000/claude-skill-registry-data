{
  "name": "llm-inference-batching-scheduler",
  "repo": "letta-ai/skills",
  "category": "design",
  "description": "Guidance for optimizing LLM inference request batching and scheduling problems. This skill applies when designing batch schedulers that minimize cost while meeting latency and padding constraints, inv",
  "source": "SkillsMP + GitHub Raw",
  "downloaded_at": "2026-01-06T08:58:18.917357Z",
  "github_path": "letta/benchmarks/trajectory-only/llm-inference-batching-scheduler",
  "github_branch": "main",
  "dir_name": "llm-inference-batching-scheduler-letta-ai-skills"
}